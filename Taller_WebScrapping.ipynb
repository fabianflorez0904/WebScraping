{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Taller de Web Scraping**\n",
    "\n",
    "## **Parte 1: Introducci칩n a Web Scraping**\n",
    "En este taller, aprender치s a realizar web scraping utilizando Python y la librer칤a `BeautifulSoup`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **Ejercicio 1: Explorar el archivo robots.txt**\n",
    "\n",
    "1. Busca el archivo `robots.txt` de una p치gina web y analiza sus reglas.\n",
    "   - Ejemplo: [https://www.wikipedia.org/robots.txt](https://www.wikipedia.org/robots.txt)\n",
    "   - Identifica qu칠 partes est치n permitidas para el scraping.\n",
    "\n",
    "## Explorar el archivo Robot.txt\n",
    "\n",
    "Dentro del archivo `robot.txt` podemos encontrar e identificar las partes que estan permitidas y cuales, para poder realizar los procesos de WebScraping. En este formato encontramos diferentes reglas relacionadas tales que:\n",
    "\n",
    "* **User-agent:** Indica a que rastreadores(como Googlebot o scrapers personalizados) se aplican las reglas.\n",
    "* **Disallow:** Especifica que partes del sitio estan prohibidas para el web scraping.\n",
    "* **Allow:** Especifica que partes del sitio estan permitidas para el scraping.\n",
    "\n",
    "### Preguntas reflexivas\n",
    "+ 쯇or qu칠 algunos sitios web bloquean el Web Scraping?\n",
    "    + El bloqueo del web scraping se da por diferentes razones, como sobrecarga del servidor, violacion de los terminos de servicio, o uso excesivo de recursos.\n",
    "    + **Sobrecarga del servidor**\n",
    "        + Los robots mal disenados pueden provocar el servidor al realizar solicitudes excesivas.\n",
    "    + **Violacion de los terminos de servicio**\n",
    "        + Los sitios web pueden bloquear los raspadores web por que violan los terminos de servicio de los sitios web.\n",
    "    + **Medidas anti-scraping**\n",
    "        +Los sitios web pueden implementar CAPTCHA para diferencia entre usuarios humanos y robot de scraping.\n",
    "\n",
    "+ 쮺u치ndo es preferible usar una API en lugar de Web Scraping?\n",
    "    + El uso de una API o de web scraping depende de la necesidad de los datos, el presupuesto, los recursos tecnologicos, y si el sitio web tiene un API.\n",
    "    + **API**\n",
    "        + Es una opcion para tener los datos estrucuturados y confiables\n",
    "        + Podemos integrar servicios de otros proveedores, como redes sociales, sistemas de pago, y geolocalizacion.\n",
    "        + Acelera el desarrollo de aplicaciones y facilita la automatizacion de tareas.\n",
    "\n",
    "    + **Web Scraping**\n",
    "        + Ofrece mayor flexibilidad y cobertura\n",
    "        + Permite extraer datos de sitios web que no tiene APIs\n",
    "        + Permite extraer informacion extra que no nos ofrece una API\n",
    "            \n",
    "+ Herramientas populares para Web Scraping en Python.\n",
    "    + La principal herramienta para el web Scraping en Python es la libreria de `Beautiful Soup`. Esta nos facilita el analisis y extraccion de datos, documentos tanto HTML y XML.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **robots.txt for www.mercadolibre.com.co**\n",
    "\n",
    "```Python\n",
    "User-agent: Amazonbot\n",
    "Disallow: /\n",
    "\n",
    "User-agent: ClaudeBot\n",
    "Disallow: /\n",
    "\n",
    "User-agent: FacebookExternalHit\n",
    "User-agent: FacebookBot\n",
    "User-agent: Twitterbot\n",
    "User-agent: LinkedInBot\n",
    "Disallow: \n",
    "\n",
    "User-agent: *\n",
    "Disallow: /HOME/\n",
    "Disallow: /gz/merch/\n",
    "Disallow: /gz/menu\n",
    "Disallow: /gz/webdevice/config\n",
    "Disallow: /gz/referidos\n",
    "Disallow: /*www.siteinfo.cf\n",
    "Disallow: /gz/cart/\n",
    "Disallow: /gz/checkout/\n",
    "Disallow: /gz/user-logged\n",
    "Disallow: /gz/shipping-selector\n",
    "Disallow: /gz/navigation/searches/last\n",
    "Disallow: /perfil/vendedor/\n",
    "Disallow: /perfil/comprador/\n",
    "Disallow: /perfil/profile/\n",
    "Disallow: /perfil/jm/profile\n",
    "Disallow: /perfil/ALEXSETHMS\n",
    "Disallow: /noindex/\n",
    "Disallow: /navigation/\n",
    "Disallow: /*itemid\n",
    "Disallow: /*/jm/item\n",
    "Disallow: /recommendations*\n",
    "Disallow: /*attributes=\n",
    "Disallow: /*quantity=\n",
    "Disallow: /org-img/html/\n",
    "Disallow: /registration?confirmation_url*\n",
    "Disallow: /home/recommendations\n",
    "Disallow: /social/\n",
    "Disallow: /adn/api*\n",
    "Disallow: /product-fe-recommendations/recommendations*\n",
    "Disallow: /*.js\n",
    "Disallow: /finditem.ml\n",
    "```\n",
    "\n",
    "### Analisis del archivo `robots.txt` de Mercado Libre\n",
    "\n",
    "1. **Bloqueo de bots especificos**\n",
    "```\n",
    "User-agent: Amazonbot\n",
    "Disallow: /\n",
    "User-agent: ClaudeBot\n",
    "Disallow: /\n",
    "\n",
    "```\n",
    "+ **Amazonbot** y **ClaudeBot** esta bloqueado, lo que significa que no puede rasrear ninguna parte del sitio.\n",
    "\n",
    "2. **Bots de redes sociales permitidos**\n",
    "```\n",
    "User-agent: FacebookExternalHit\n",
    "User-agent: FacebookBot\n",
    "User-agent: Twitterbot\n",
    "User-agent: LinkedInBot\n",
    "Disallow: \n",
    "```\n",
    "3. **Restricciones generales para todos los robots**\n",
    "```\n",
    "User-agent: *\n",
    "Disallow: /HOME/\n",
    "Disallow: /gz/merch/\n",
    "Disallow: /gz/menu\n",
    "Disallow: /gz/webdevice/config\n",
    "Disallow: /gz/referidos\n",
    "Disallow: /*www.siteinfo.cf\n",
    "Disallow: /gz/cart/\n",
    "Disallow: /gz/checkout/\n",
    "Disallow: /gz/user-logged\n",
    "Disallow: /gz/shipping-selector\n",
    "Disallow: /gz/navigation/searches/last\n",
    "Disallow: /perfil/vendedor/\n",
    "Disallow: /perfil/comprador/\n",
    "Disallow: /perfil/profile/\n",
    "Disallow: /perfil/jm/profile\n",
    "Disallow: /perfil/ALEXSETHMS\n",
    "Disallow: /noindex/\n",
    "Disallow: /navigation/\n",
    "Disallow: /*itemid\n",
    "Disallow: /*/jm/item\n",
    "Disallow: /recommendations*\n",
    "Disallow: /*attributes=\n",
    "Disallow: /*quantity=\n",
    "Disallow: /org-img/html/\n",
    "Disallow: /registration?confirmation_url*\n",
    "Disallow: /home/recommendations\n",
    "Disallow: /social/\n",
    "Disallow: /adn/api*\n",
    "Disallow: /product-fe-recommendations/recommendations*\n",
    "Disallow: /*.js\n",
    "Disallow: /finditem.ml\n",
    "```\n",
    "+ Se prohible el acceso a secciones sensibles como:\n",
    "    + Carrito de compras y checkout(`/gz/cart/`, `/gz/checkout/`)\n",
    "    + Paginas de usuarios (`/perfil/vendedor/`, `/perfil/comprador/`, etc)\n",
    "    + Paginas de navegacion y busqueda interna (`/navigation/`,`/gz/navigation/searchez/last`)\n",
    "    + Recomendaciones y API internas  (`/recommendations*`, `/adn/api*`, `/product-fe-recommendations/recommendations*`)\n",
    "\n",
    "+ Mercado libre restringe el acceso a datos sensibles y dinamicos\n",
    "+ Los bots de redes sociales tienen acceso total, lo que facilita compartir y promocionar productos\n",
    "+ El bloqueo de robots especificos como Amazon o Claude, posiblemente sera para evitar que lean y analisen el mercado con esta informacion.\n",
    "+ Al estar bloqueado el acceso a los archivos `.js` y las URLs de configuracion interna, se protegen de que el sitio sea replicado o que se analize con profundidad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Taller de Web Scraping**\n",
    "\n",
    "## **Parte 1: Introducci칩n a Web Scraping**\n",
    "\n",
    "### **Ejercicio 1: Explorar el archivo robots.txt**\n",
    "\n",
    "1. Busca el archivo `robots.txt` de una p치gina web y analiza sus reglas.\n",
    "   - Ejemplo: [https://www.wikipedia.org/robots.txt](https://www.wikipedia.org/robots.txt)\n",
    "   - Identifica qu칠 partes est치n permitidas para el scraping.\n",
    "\n",
    "2. Explicaci칩n de las reglas:\n",
    "   - **User-agent**: Indica a qu칠 rastreadores (como Googlebot o scrapers personalizados) se aplican las reglas.\n",
    "   - **Disallow**: Especifica qu칠 partes del sitio est치n prohibidas para el web scraping.\n",
    "   - **Allow**: Especifica qu칠 partes del sitio est치n permitidas para el scraping.\n",
    "\n",
    "### **Preguntas reflexivas**\n",
    "- 쯇or qu칠 algunos sitios web bloquean el Web Scraping?\n",
    "- 쮺u치ndo es preferible usar una API en lugar de Web Scraping?\n",
    "- Herramientas populares para Web Scraping en Python.\n",
    "\n",
    "## **Parte 2: Implementaci칩n de Web Scraping en Python**\n",
    "\n",
    "### **Ejercicio 2: Extraer datos de una p치gina web**\n",
    "\n",
    "1. Utiliza `requests` y `BeautifulSoup` para extraer contenido de una p치gina web.\n",
    "2. Identifica elementos HTML relevantes (como etiquetas `<div>`, `<p>`, `<a>`).\n",
    "3. Extrae y almacena la informaci칩n en un formato estructurado (CSV o JSON).\n",
    "\n",
    "**C칩digo base:**\n",
    "```python\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://ejemplo.com\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Extraer informaci칩n relevante\n",
    "data = soup.find_all(\"p\")\n",
    "for item in data:\n",
    "    print(item.text)\n",
    "```\n",
    "\n",
    "### **Ejercicio 3: Scraping avanzado con Selenium**\n",
    "\n",
    "1. Usa `Selenium` para interactuar con p치ginas din치micas.\n",
    "2. Extrae informaci칩n que se carga mediante JavaScript.\n",
    "3. Simula acciones de usuario, como clics y desplazamientos.\n",
    "\n",
    "**C칩digo base:**\n",
    "```python\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "\n",
    "# Configurar el driver\n",
    "options = webdriver.ChromeOptions()\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "driver.get(\"https://ejemplo.com\")\n",
    "time.sleep(5)  # Esperar carga de la p치gina\n",
    "\n",
    "data = driver.find_element(By.TAG_NAME, \"p\").text\n",
    "print(data)\n",
    "\n",
    "driver.quit()\n",
    "```\n",
    "\n",
    "## **Parte 3: 칄tica y buenas pr치cticas en Web Scraping**\n",
    "\n",
    "### **Ejercicio 4: Respetar los t칠rminos de uso y evitar bloqueos**\n",
    "- Revisar el `robots.txt` antes de hacer scraping.\n",
    "- Implementar tiempos de espera (`time.sleep()`) para no sobrecargar servidores.\n",
    "- Usar `headers` para simular una petici칩n leg칤tima.\n",
    "- Preferir APIs cuando est칠n disponibles.\n",
    "\n",
    "**Ejemplo de petici칩n con `headers`:**\n",
    "```python\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "response = requests.get(url, headers=headers)\n",
    "```\n",
    "\n",
    "### **Ejercicio 5: Guardar y analizar los datos extra칤dos**\n",
    "1. Almacenar los datos en CSV o JSON.\n",
    "2. Analizar la informaci칩n con `pandas`.\n",
    "\n",
    "**Ejemplo de almacenamiento en CSV:**\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\"Datos\": data})\n",
    "df.to_csv(\"datos.csv\", index=False)\n",
    "```\n",
    "\n",
    "## **Conclusi칩n**\n",
    "Este taller proporciona una introducci칩n a Web Scraping con Python, cubriendo desde la exploraci칩n de `robots.txt` hasta la implementaci칩n con `BeautifulSoup` y `Selenium`. 춰Ahora es tu turno de practicar! 游\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estructura de un Documento Markdown\n",
    "\n",
    "Markdown es un lenguaje de marcado ligero que permite formatear texto de manera sencilla. A continuaci칩n, se presenta la estructura b치sica de un documento en Markdown.\n",
    "\n",
    "## 1. Encabezados\n",
    "\n",
    "Se utilizan `#` para definir encabezados de distintos niveles:\n",
    "\n",
    "```markdown\n",
    "# Encabezado de nivel 1\n",
    "## Encabezado de nivel 2\n",
    "### Encabezado de nivel 3\n",
    "#### Encabezado de nivel 4\n",
    "##### Encabezado de nivel 5\n",
    "###### Encabezado de nivel 6\n",
    "```\n",
    "\n",
    "## 2. P치rrafos y Saltos de L칤nea\n",
    "\n",
    "Para crear un p치rrafo, simplemente escribe el texto dejando una l칤nea en blanco entre p치rrafos.\n",
    "Para forzar un salto de l칤nea dentro de un p치rrafo, a침ade dos espacios al final de la l칤nea.\n",
    "\n",
    "## 3. Estilos de Texto\n",
    "\n",
    "Puedes aplicar diferentes estilos al texto:\n",
    "\n",
    "```markdown\n",
    "**Negrita** (doble asterisco o doble guion bajo): **Ejemplo** o __Ejemplo__\n",
    "*Cursiva* (un asterisco o un guion bajo): *Ejemplo* o _Ejemplo_\n",
    "~~Tachado~~ (doble tilde): ~~Ejemplo~~\n",
    "```\n",
    "\n",
    "## 4. Listas\n",
    "\n",
    "### Listas no ordenadas\n",
    "Se crean usando `-`, `*` o `+`:\n",
    "\n",
    "```markdown\n",
    "- Elemento 1\n",
    "- Elemento 2\n",
    "  - Subelemento 2.1\n",
    "  - Subelemento 2.2\n",
    "```\n",
    "\n",
    "### Listas ordenadas\n",
    "Se crean usando n칰meros seguidos de un punto:\n",
    "\n",
    "```markdown\n",
    "1. Elemento 1\n",
    "2. Elemento 2\n",
    "   1. Subelemento 2.1\n",
    "   2. Subelemento 2.2\n",
    "```\n",
    "\n",
    "## 5. Enlaces\n",
    "\n",
    "```markdown\n",
    "[Texto del enlace](https://ejemplo.com)\n",
    "```\n",
    "\n",
    "Ejemplo: [Visitar Google](https://www.google.com)\n",
    "\n",
    "## 6. Im치genes\n",
    "\n",
    "```markdown\n",
    "![Texto alternativo](https://via.placeholder.com/150 \"T칤tulo opcional\")\n",
    "```\n",
    "\n",
    "## 7. Citas\n",
    "\n",
    "Se crean usando `>` al inicio de la l칤nea:\n",
    "\n",
    "```markdown\n",
    "> Esto es una cita en Markdown.\n",
    "```\n",
    "\n",
    "## 8. C칩digo\n",
    "\n",
    "Para incluir c칩digo en l칤nea, usa comillas invertidas `` `c칩digo` ``.\n",
    "Para bloques de c칩digo, usa triple comilla invertida:\n",
    "\n",
    "```markdown\n",
    "```\n",
    "print(\"Hola, Markdown!\")\n",
    "```\n",
    "```\n",
    "\n",
    "## 9. Tablas\n",
    "\n",
    "```markdown\n",
    "| Encabezado 1 | Encabezado 2 | Encabezado 3 |\n",
    "|-------------|-------------|-------------|\n",
    "| Dato 1      | Dato 2      | Dato 3      |\n",
    "| Dato 4      | Dato 5      | Dato 6      |\n",
    "```\n",
    "\n",
    "## 10. L칤neas Horizontales\n",
    "\n",
    "Se crean con tres guiones `---`, tres asteriscos `***` o tres guiones bajos `___`.\n",
    "\n",
    "```markdown\n",
    "---\n",
    "```\n",
    "\n",
    "## 11. Checkbox (Listas de Tareas)\n",
    "\n",
    "```markdown\n",
    "- [ ] Tarea pendiente\n",
    "- [x] Tarea completada\n",
    "```\n",
    "\n",
    "## Conclusi칩n\n",
    "\n",
    "Markdown es una herramienta poderosa y f치cil de usar para formatear texto de manera eficiente. Con esta gu칤a, puedes comenzar a escribir documentos en Markdown de manera estructurada y organizada.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Taller de Web Scraping**\n",
    "\n",
    "## **Parte 1: Introducción a Web Scraping**\n",
    "En este taller, aprenderás a realizar web scraping utilizando Python y la librería `BeautifulSoup`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **Ejercicio 1: Explorar el archivo robots.txt**\n",
    "\n",
    "1. Busca el archivo `robots.txt` de una página web y analiza sus reglas.\n",
    "   - Ejemplo: [https://www.wikipedia.org/robots.txt](https://www.wikipedia.org/robots.txt)\n",
    "   - Identifica qué partes están permitidas para el scraping.\n",
    "\n",
    "## Explorar el archivo Robot.txt\n",
    "\n",
    "Dentro del archivo `robot.txt` podemos encontrar e identificar las partes que estan permitidas y cuales, para poder realizar los procesos de WebScraping. En este formato encontramos diferentes reglas relacionadas tales que:\n",
    "\n",
    "* **User-agent:** Indica a que rastreadores(como Googlebot o scrapers personalizados) se aplican las reglas.\n",
    "* **Disallow:** Especifica que partes del sitio estan prohibidas para el web scraping.\n",
    "* **Allow:** Especifica que partes del sitio estan permitidas para el scraping.\n",
    "\n",
    "### Preguntas reflexivas\n",
    "+ ¿Por qué algunos sitios web bloquean el Web Scraping?\n",
    "    + El bloqueo del web scraping se da por diferentes razones, como sobrecarga del servidor, violacion de los terminos de servicio, o uso excesivo de recursos.\n",
    "    + **Sobrecarga del servidor**\n",
    "        + Los robots mal disenados pueden provocar el servidor al realizar solicitudes excesivas.\n",
    "    + **Violacion de los terminos de servicio**\n",
    "        + Los sitios web pueden bloquear los raspadores web por que violan los terminos de servicio de los sitios web.\n",
    "    + **Medidas anti-scraping**\n",
    "        +Los sitios web pueden implementar CAPTCHA para diferencia entre usuarios humanos y robot de scraping.\n",
    "\n",
    "+ ¿Cuándo es preferible usar una API en lugar de Web Scraping?\n",
    "    + El uso de una API o de web scraping depende de la necesidad de los datos, el presupuesto, los recursos tecnologicos, y si el sitio web tiene un API.\n",
    "    + **API**\n",
    "        + Es una opcion para tener los datos estrucuturados y confiables\n",
    "        + Podemos integrar servicios de otros proveedores, como redes sociales, sistemas de pago, y geolocalizacion.\n",
    "        + Acelera el desarrollo de aplicaciones y facilita la automatizacion de tareas.\n",
    "\n",
    "    + **Web Scraping**\n",
    "        + Ofrece mayor flexibilidad y cobertura\n",
    "        + Permite extraer datos de sitios web que no tiene APIs\n",
    "        + Permite extraer informacion extra que no nos ofrece una API\n",
    "            \n",
    "+ Herramientas populares para Web Scraping en Python.\n",
    "    + La principal herramienta para el web Scraping en Python es la libreria de `Beautiful Soup`. Esta nos facilita el analisis y extraccion de datos, documentos tanto HTML y XML.\n",
    "\n",
    "### **robots.txt for www.mercadolibre.com.co**\n",
    "\n",
    "```Python\n",
    "User-agent: Amazonbot\n",
    "Disallow: /\n",
    "\n",
    "User-agent: ClaudeBot\n",
    "Disallow: /\n",
    "\n",
    "User-agent: FacebookExternalHit\n",
    "User-agent: FacebookBot\n",
    "User-agent: Twitterbot\n",
    "User-agent: LinkedInBot\n",
    "Disallow: \n",
    "\n",
    "User-agent: *\n",
    "Disallow: /HOME/\n",
    "Disallow: /gz/merch/\n",
    "Disallow: /gz/menu\n",
    "Disallow: /gz/webdevice/config\n",
    "Disallow: /gz/referidos\n",
    "Disallow: /*www.siteinfo.cf\n",
    "Disallow: /gz/cart/\n",
    "Disallow: /gz/checkout/\n",
    "Disallow: /gz/user-logged\n",
    "Disallow: /gz/shipping-selector\n",
    "Disallow: /gz/navigation/searches/last\n",
    "Disallow: /perfil/vendedor/\n",
    "Disallow: /perfil/comprador/\n",
    "Disallow: /perfil/profile/\n",
    "Disallow: /perfil/jm/profile\n",
    "Disallow: /perfil/ALEXSETHMS\n",
    "Disallow: /noindex/\n",
    "Disallow: /navigation/\n",
    "Disallow: /*itemid\n",
    "Disallow: /*/jm/item\n",
    "Disallow: /recommendations*\n",
    "Disallow: /*attributes=\n",
    "Disallow: /*quantity=\n",
    "Disallow: /org-img/html/\n",
    "Disallow: /registration?confirmation_url*\n",
    "Disallow: /home/recommendations\n",
    "Disallow: /social/\n",
    "Disallow: /adn/api*\n",
    "Disallow: /product-fe-recommendations/recommendations*\n",
    "Disallow: /*.js\n",
    "Disallow: /finditem.ml\n",
    "```\n",
    "\n",
    "### Analisis del archivo `robots.txt` de Mercado Libre\n",
    "\n",
    "1. **Bloqueo de bots especificos**\n",
    "```\n",
    "User-agent: Amazonbot\n",
    "Disallow: /\n",
    "User-agent: ClaudeBot\n",
    "Disallow: /\n",
    "\n",
    "```\n",
    "+ **Amazonbot** y **ClaudeBot** esta bloqueado, lo que significa que no puede rasrear ninguna parte del sitio.\n",
    "\n",
    "2. **Bots de redes sociales permitidos**\n",
    "```\n",
    "User-agent: FacebookExternalHit\n",
    "User-agent: FacebookBot\n",
    "User-agent: Twitterbot\n",
    "User-agent: LinkedInBot\n",
    "Disallow: \n",
    "```\n",
    "3. **Restricciones generales para todos los robots**\n",
    "```\n",
    "User-agent: *\n",
    "Disallow: /HOME/\n",
    "Disallow: /gz/merch/\n",
    "Disallow: /gz/menu\n",
    "Disallow: /gz/webdevice/config\n",
    "Disallow: /gz/referidos\n",
    "Disallow: /*www.siteinfo.cf\n",
    "Disallow: /gz/cart/\n",
    "Disallow: /gz/checkout/\n",
    "Disallow: /gz/user-logged\n",
    "Disallow: /gz/shipping-selector\n",
    "Disallow: /gz/navigation/searches/last\n",
    "Disallow: /perfil/vendedor/\n",
    "Disallow: /perfil/comprador/\n",
    "Disallow: /perfil/profile/\n",
    "Disallow: /perfil/jm/profile\n",
    "Disallow: /perfil/ALEXSETHMS\n",
    "Disallow: /noindex/\n",
    "Disallow: /navigation/\n",
    "Disallow: /*itemid\n",
    "Disallow: /*/jm/item\n",
    "Disallow: /recommendations*\n",
    "Disallow: /*attributes=\n",
    "Disallow: /*quantity=\n",
    "Disallow: /org-img/html/\n",
    "Disallow: /registration?confirmation_url*\n",
    "Disallow: /home/recommendations\n",
    "Disallow: /social/\n",
    "Disallow: /adn/api*\n",
    "Disallow: /product-fe-recommendations/recommendations*\n",
    "Disallow: /*.js\n",
    "Disallow: /finditem.ml\n",
    "```\n",
    "+ Se prohible el acceso a secciones sensibles como:\n",
    "    + Carrito de compras y checkout(`/gz/cart/`, `/gz/checkout/`)\n",
    "    + Paginas de usuarios (`/perfil/vendedor/`, `/perfil/comprador/`, etc)\n",
    "    + Paginas de navegacion y busqueda interna (`/navigation/`,`/gz/navigation/searchez/last`)\n",
    "    + Recomendaciones y API internas  (`/recommendations*`, `/adn/api*`, `/product-fe-recommendations/recommendations*`)\n",
    "\n",
    "+ Mercado libre restringe el acceso a datos sensibles y dinamicos\n",
    "+ Los bots de redes sociales tienen acceso total, lo que facilita compartir y promocionar productos\n",
    "+ El bloqueo de robots especificos como Amazon o Claude, posiblemente sera para evitar que lean y analisen el mercado con esta informacion.\n",
    "+ Al estar bloqueado el acceso a los archivos `.js` y las URLs de configuracion interna, se protegen de que el sitio sea replicado o que se analize con profundidad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Parte 2: Implementación de Web Scraping en Python**\n",
    "\n",
    "### **Ejercicio 2: Extraer datos de una página web**\n",
    "\n",
    "1. Utiliza `requests` y `BeautifulSoup` para extraer contenido de una página web.\n",
    "2. Identifica elementos HTML relevantes (como etiquetas `<div>`, `<p>`, `<a>`).\n",
    "3. Extrae y almacena la información en un formato estructurado (CSV o JSON).\n",
    "\n",
    "**Código base:**\n",
    "```python\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://ejemplo.com\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Extraer información relevante\n",
    "data = soup.find_all(\"p\")\n",
    "for item in data:\n",
    "    print(item.text)\n",
    "```\n",
    "Vamos a realizar la extraccion de la informacion de la pagina de noticias `elpais.com` previa revision del documento `robots.txt` y no encontrar ninguna restriccion frente a esta practica a los robots de pruebas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3551064998.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[52]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mpip install requests beautifulsoup4\u001b[39m\n        ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "pip install requests beautifulsoup4\n",
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Conexion exitosa a https://elpais.com/tecnologia/\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL de la pagina de noticias\n",
    "url = \"https://elpais.com/tecnologia/\"\n",
    "headers = {\"User-Agent\": \"Chrome/120.0.0.0\"}\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(f\"✅ Conexion exitosa a {url}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "👽 Cantidad de noticias extraidas 21\n",
      "1. Descifrando el consumo de agua de la IA: así oculta Amazon cuánto bebe su nube en España\n",
      "2. El Gobierno propone blindar a los actores contra la inteligencia artificial\n",
      "3. Por qué Deepseek y Bluesky son las primeras grietas en el poder de la tecnocasta\n",
      "4. Las mafias del cibercrimen cuentan con un ejército de más de 250.000 esclavos sometidos a torturas, extorsiones y violaciones\n",
      "5. Un portátil solar, lentillas inteligentes y otras extravagancias del MWC\n",
      "6. Samsung expone cómo la IA cambiará la forma de usar el ‘smartphone’\n",
      "7. Emilio Carrizosa, matemático: “El liderazgo de la inteligencia artificial no lo tienen ahora mismo los gobiernos, sino empresas privadas”\n",
      "8. Protección de Datos impone una multa de un millón de euros a LaLiga por recoger datos biométricos de los espectadores\n",
      "9. Cómo la política de Trump sobre IA beneficiará a las grandes empresas\n",
      "10. Los memes son una herramienta fundamental para las comunidades extremistas y para teorías de la conspiración\n",
      "11. Los robots del MWC se apuntan a la inteligencia artificial\n",
      "12. “La guerra supone estrés constante para todos”: jóvenes ucranios desarrollan tecnología para tratar la salud mental y fomentar la natalidad\n",
      "13. El miedo al hombre blandengue vuelve a Silicon Valley\n",
      "14. Los móviles del MWC 2025, cada vez menos y menos deslumbrantes\n",
      "15. “Yo vivo de mi voz. Si me la emulan, estoy acabado”: Los actores de doblaje se movilizan contra la IA\n",
      "16. ‘Fachatubers’: la extrema derecha se cuela en las pantallas de los más jóvenes\n",
      "17. ¿Quién va ganando la carrera de la IA generativa?\n",
      "18. Oda al Autotune: himnos para la generación de cristal\n",
      "19. El legionario terraplanista de Masterchef y otros `despertares de la conciencia´\n",
      "20. Pódcast | El apoteósico final de 'Titania', la ficción sonora más premiada del año\n",
      "21. Cómo descartar definitivamente que Brad Pitt no es quien habla por videollamada, aunque lo parezca\n"
     ]
    }
   ],
   "source": [
    "# Parseamos el contenido de la pagina con BeatifulSoup\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Tras previa busqueda de los elementos HTML que contienen los titulos de las noticias\n",
    "# Estraemos los titulos con el tag h2\n",
    "titulos = soup.find_all(\"h2\") # nos devuelve una lista con todos los titulos\n",
    "\n",
    "print(f\"👽 Cantidad de noticias extraidas {len(titulos)}\")\n",
    "# Iteramos la lista de titulos\n",
    "for i,titulo in enumerate(titulos,start=1):\n",
    "    print(f\"{i}. {titulo.text.strip()}\") # Strip() nos ayuda a eliminar espacios al inico y al final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 Extraemos las noticias que contengan algo relacionado a la IA\n",
      "1. Samsung expone cómo la IA cambiará la forma de usar el ‘smartphone’\n",
      "2. Cómo la política de Trump sobre IA beneficiará a las grandes empresas\n",
      "3. ¿Quién va ganando la carrera de la IA generativa?\n"
     ]
    }
   ],
   "source": [
    "print(f\"🤖 Extraemos las noticias que contengan algo relacionado a la IA\")\n",
    "i = 0\n",
    "for titulo in titulos:\n",
    "    if \" IA \" in titulo.text.upper() or \"inteligencia artificial\" in titulo.text.upper():\n",
    "        i += 1\n",
    "        print(f\"{i}. {titulo.text.strip()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Parte 3: Scraping de Datos en Tablas**\n",
    "\n",
    "### Conceptos clave\n",
    "\n",
    "- **Uso de `find_all` para extraer datos tabulares**: En esta sección, aprenderás a extraer datos de tablas HTML utilizando la función `find_all` de BeautifulSoup. Las tablas en HTML están estructuradas con etiquetas como `<table>`, `<tr>` (filas), `<th>` (encabezados) y `<td>` (celdas de datos).\n",
    "\n",
    "### Ejercicio 3: Extraer datos de una tabla de Wikipedia\n",
    "\n",
    "1. **Busca una página de Wikipedia con una tabla de datos**: Por ejemplo, puedes usar la página de la lista de países por PIB nominal: [Lista de países por PIB (nominal)](https://es.wikipedia.org/wiki/Lista_de_pa%C3%ADses_por_PIB_(nominal)).\n",
    "\n",
    "2. **Modifica el siguiente código para extraer los datos de una tabla específica**:\n",
    "\n",
    "   ```python\n",
    "   import requests\n",
    "   from bs4 import BeautifulSoup\n",
    "\n",
    "   # URL de la página de Wikipedia con la tabla\n",
    "   url = \"https://es.wikipedia.org/wiki/Lista_de_pa%C3%ADses_por_PIB_(nominal)\"\n",
    "   \n",
    "   # Realizar la solicitud HTTP\n",
    "   response = requests.get(url)\n",
    "   \n",
    "   # Parsear el contenido HTML con BeautifulSoup\n",
    "   soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "   \n",
    "   # Encontrar la tabla por su clase (en este caso, \"wikitable\")\n",
    "   tabla = soup.find(\"table\", {\"class\": \"wikitable\"})\n",
    "   \n",
    "   # Extraer todas las filas de la tabla\n",
    "   filas = tabla.find_all(\"tr\")\n",
    "   \n",
    "   # Iterar sobre cada fila y extraer los datos de las celdas\n",
    "   for fila in filas:\n",
    "       columns = fila.find_all(\"td\")\n",
    "       datos = [col.text.strip() for col in columns]\n",
    "       print(datos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Conexion exitosa a https://es.wikipedia.org/wiki/Poblaci%C3%B3n_mundial\n"
     ]
    }
   ],
   "source": [
    "# URL de la pagina de wikipedia\n",
    "url = \"https://es.wikipedia.org/wiki/Poblaci%C3%B3n_mundial\"\n",
    "headers = {\"User-Agent\": \"Chrome/120.0.0.0\"}\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(f\"✅ Conexion exitosa a {url}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "👽 Cantidad de tablas extraidas 19\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Puesto</td>\n",
       "      <td>País o territorio dependiente</td>\n",
       "      <td>Población  (proyección 2024)[15]​</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.º</td>\n",
       "      <td>India</td>\n",
       "      <td>1 450 935 779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.º</td>\n",
       "      <td>China</td>\n",
       "      <td>1 419 321 279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.º</td>\n",
       "      <td>Estados Unidos</td>\n",
       "      <td>345 426 567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.º</td>\n",
       "      <td>Indonesia</td>\n",
       "      <td>283 487 932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.º</td>\n",
       "      <td>Pakistán</td>\n",
       "      <td>251 269 158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6.º</td>\n",
       "      <td>Nigeria</td>\n",
       "      <td>232 679 482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7.º</td>\n",
       "      <td>Brasil</td>\n",
       "      <td>211 998 564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8.º</td>\n",
       "      <td>Bangladés</td>\n",
       "      <td>173 562 367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9.º</td>\n",
       "      <td>Rusia</td>\n",
       "      <td>144 820 423</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0                              1                                  2\n",
       "0  Puesto  País o territorio dependiente  Población  (proyección 2024)[15]​\n",
       "1     1.º                          India                      1 450 935 779\n",
       "2     2.º                          China                      1 419 321 279\n",
       "3     3.º                 Estados Unidos                        345 426 567\n",
       "4     4.º                      Indonesia                        283 487 932\n",
       "5     5.º                       Pakistán                        251 269 158\n",
       "6     6.º                        Nigeria                        232 679 482\n",
       "7     7.º                         Brasil                        211 998 564\n",
       "8     8.º                      Bangladés                        173 562 367\n",
       "9     9.º                          Rusia                        144 820 423"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Parseamos el contenido de la pagina con BeatifulSoup\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "tablas = soup.find_all(\"table\",{\"class\":\"wikitable\"}) \n",
    "\n",
    "print(f\"👽 Cantidad de tablas extraidas {len(tablas)}\")\n",
    "\n",
    "tabla_objetivo = tablas[3]\n",
    "\n",
    "titulos = tabla_objetivo.find_all(\"th\")\n",
    "filas = tabla_objetivo.find_all(\"tr\")\n",
    "\n",
    "datos = []\n",
    "\n",
    "fila_titulos = [tl.text.strip() for tl in titulos]\n",
    "datos.append(fila_titulos)\n",
    "\n",
    "for fila in filas:\n",
    "    columnas = fila.find_all(\"td\")\n",
    "    fila_datos = [col.text.strip() for col in columnas]\n",
    "    if fila_datos:\n",
    "        datos.append(fila_datos)\n",
    "\n",
    "df = pd.DataFrame(datos)\n",
    "\n",
    "df.to_csv(\"paises_sur_america.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "# Para este caso practico traemos los 10 paises con mas poblacion\n",
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Parte 4: Buenas Prácticas y Ética en Web Scraping**\n",
    "\n",
    "### Conceptos clave\n",
    "\n",
    "- **Respetar `robots.txt`**: El archivo `robots.txt` es un estándar utilizado por los sitios web para indicar a los bots (como los de Web Scraping) qué partes del sitio pueden o no pueden ser accedidas. Es importante respetar estas reglas para evitar problemas legales o técnicos.\n",
    "\n",
    "- **No sobrecargar servidores con muchas solicitudes en poco tiempo**: Realizar demasiadas solicitudes en un corto período de tiempo puede sobrecargar el servidor del sitio web, lo que puede llevar a bloqueos o incluso a la denegación de servicio. Es recomendable implementar pausas entre solicitudes.\n",
    "\n",
    "- **Usar cabeceras HTTP adecuadas (`User-Agent`)**: El `User-Agent` es una cabecera HTTP que identifica el navegador o herramienta que está realizando la solicitud. Es importante usar un `User-Agent` válido para evitar ser bloqueado por el servidor.\n",
    "\n",
    "- **Preferir APIs oficiales cuando estén disponibles**: Si el sitio web ofrece una API oficial, es preferible usarla en lugar de realizar Web Scraping. Las APIs están diseñadas para proporcionar datos de manera estructurada y suelen ser más eficientes y legales.\n",
    "\n",
    "### Ejercicio 4: Rotación de User-Agent\n",
    "\n",
    "Modifica tu código de BeautifulSoup para incluir un `User-Agent` diferente en cada solicitud. Aquí tienes un ejemplo de cómo hacerlo:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.✅ Conexion exitosa a https://elpais.com/tecnologia/\n",
      "1.✅ Conexion exitosa a https://elpais.com/tecnologia/\n",
      "2.✅ Conexion exitosa a https://elpais.com/tecnologia/\n",
      "3.✅ Conexion exitosa a https://elpais.com/tecnologia/\n",
      "4.✅ Conexion exitosa a https://elpais.com/tecnologia/\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "\n",
    "# Lista de User-Agents para rotar\n",
    "user_agents = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Safari/605.1.15\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0\",\n",
    "    \"Mozilla/5.0 (iPhone; CPU iPhone OS 14_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0 Mobile/15E148 Safari/604.1\"\n",
    "]\n",
    "\n",
    "# Seleccionar un User-Agent aleatorio\n",
    "headers = {\n",
    "    \"User-Agent\": random.choice(user_agents)\n",
    "}\n",
    "\n",
    "# URL de la página a scrapear\n",
    "url = \"https://elpais.com/tecnologia/\"\n",
    "\n",
    "# Realizar la solicitud HTTP con el User-Agent rotado\n",
    "\n",
    "for i in range(5):\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        print(f\"{i}.✅ Conexion exitosa a {url}\")\n",
    "    else:\n",
    "        print(f\"{i}.⚠️ Conexion codigo a {response.status_code}\")\n",
    "        \n",
    "# Parsear el contenido HTML con BeautifulSoup\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preguntas\n",
    "+ ¿Por qué es importante rotar User-Agents?\n",
    "\n",
    "    + Rotar el `User-Agent` es imporante para evitar que el servidor detecte que el webscraping que estamos realizando proviene de un robot y no nos vaya a bloquear los servicios o el acceso. Al cambiar de `User Agent` Simulamos que las solicitudes provienen de navegadores diferentes en cada peticion. \n",
    "\n",
    "\n",
    "+ ¿Qué pasa si realizamos muchas solicitudes a un sitio sin control?\n",
    "\n",
    "    +Si Realizamos muchas solicitudes a un sitio en un corto tiempo podemos afectar negativamente el servicio que estamos consultando.\n",
    "\n",
    "    - Podemos resultar con un bloqueo temporal o que nos beten permantente desde nuestra IP\n",
    "    - Puede resultar en un ataque de denegacion de servicios (DoS) para otros usuarios.\n",
    "    - Si la pagina o servicio consultado lo detecta como un ataque real pueden emprender acciones legales contra el ejecutante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Parte 5: Uso de IA en Web Scraping**\n",
    "\n",
    "### Conceptos clave\n",
    "\n",
    "- **¿Cómo puede ayudar la IA en el análisis de datos obtenidos mediante Web Scraping?**\n",
    "  La Inteligencia Artificial (IA) puede ser de gran ayuda para procesar y analizar grandes volúmenes de datos extraídos mediante Web Scraping. Algunas aplicaciones incluyen:\n",
    "  - **Resumen automático de texto**: La IA puede resumir artículos o noticias largas en pocas frases.\n",
    "  - **Clasificación de contenido**: La IA puede clasificar datos en categorías específicas, como noticias por temática.\n",
    "  - **Análisis de sentimientos**: La IA puede determinar el tono emocional de un texto (positivo, negativo, neutral).\n",
    "  - **Extracción de entidades**: La IA puede identificar nombres, lugares, fechas y otros elementos clave en un texto.\n",
    "\n",
    "- **Introducción a herramientas de IA**:\n",
    "  - **GPT (Generative Pre-trained Transformer)**: Modelos de lenguaje como GPT-4 pueden generar texto, resumir contenido y realizar tareas de procesamiento de lenguaje natural (NLP).\n",
    "  - **Hugging Face**: Una plataforma que ofrece modelos preentrenados para tareas de NLP, como clasificación de texto y generación de resúmenes.\n",
    "  - **Google Vision API**: Herramienta de Google para análisis de imágenes, que puede ser útil si el scraping incluye contenido visual.\n",
    "\n",
    "### Ejercicio 5: Resumen Automático de Contenido Web\n",
    "\n",
    "1. **Extraer el texto de un artículo de noticias usando Web Scraping**.\n",
    "2. **Enviar el texto a la API de OpenAI (GPT-4) o Google Gemini para obtener un resumen**.\n",
    "3. **Comparar el resumen generado por IA con el contenido original**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: openai in c:\\users\\fabfl\\appdata\\roaming\\python\\python312\\site-packages (1.65.4)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\fabfl\\appdata\\roaming\\python\\python312\\site-packages (from openai) (4.8.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\fabfl\\appdata\\roaming\\python\\python312\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\fabfl\\appdata\\roaming\\python\\python312\\site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\fabfl\\appdata\\roaming\\python\\python312\\site-packages (from openai) (0.8.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\fabfl\\appdata\\roaming\\python\\python312\\site-packages (from openai) (2.10.6)\n",
      "Requirement already satisfied: sniffio in c:\\users\\fabfl\\appdata\\roaming\\python\\python312\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\fabfl\\appdata\\roaming\\python\\python312\\site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\fabfl\\appdata\\roaming\\python\\python312\\site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\fabfl\\appdata\\roaming\\python\\python312\\site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\fabfl\\appdata\\roaming\\python\\python312\\site-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\fabfl\\appdata\\roaming\\python\\python312\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\fabfl\\appdata\\roaming\\python\\python312\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\fabfl\\appdata\\roaming\\python\\python312\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\fabfl\\appdata\\roaming\\python\\python312\\site-packages (from pydantic<3,>=1.9.0->openai) (2.27.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\fabfl\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Conexion exitosa a https://elpais.com/tecnologia/2025-03-04/los-memes-son-una-herramienta-fundamental-para-las-comunidades-extremistas-y-para-teorias-de-la-conspiracion.html\n"
     ]
    },
    {
     "ename": "APIRemovedInV1",
     "evalue": "\n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAPIRemovedInV1\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 39\u001b[39m\n\u001b[32m     36\u001b[39m openai.api_key = os.getenv(\u001b[33m\"\u001b[39m\u001b[33mOPENAI_API_KEY\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# Realiza la solicitud al modelo de chat\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m respuesta = \u001b[43mopenai\u001b[49m\u001b[43m.\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mgpt-3.5-turbo\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msystem\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mEres un periodista que resume noticias\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtexto\u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# Imprime la respuesta del modelo\u001b[39;00m\n\u001b[32m     48\u001b[39m \u001b[38;5;28mprint\u001b[39m(respuesta.choices[\u001b[32m0\u001b[39m].message[\u001b[33m'\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\openai\\lib\\_old_api.py:39\u001b[39m, in \u001b[36mAPIRemovedInV1Proxy.__call__\u001b[39m\u001b[34m(self, *_args, **_kwargs)\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *_args: Any, **_kwargs: Any) -> Any:\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m APIRemovedInV1(symbol=\u001b[38;5;28mself\u001b[39m._symbol)\n",
      "\u001b[31mAPIRemovedInV1\u001b[39m: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import openai\n",
    "import random\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Cargar las variables de entorno desde el archivo .env\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "user_agents = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Safari/605.1.15\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0\",\n",
    "    \"Mozilla/5.0 (iPhone; CPU iPhone OS 14_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0 Mobile/15E148 Safari/604.1\"\n",
    "]\n",
    "\n",
    "# Seleccionar un User-Agent aleatorio\n",
    "headers = {\n",
    "    \"User-Agent\": random.choice(user_agents)\n",
    "}\n",
    "\n",
    "url = \"https://elpais.com/tecnologia/2025-03-04/los-memes-son-una-herramienta-fundamental-para-las-comunidades-extremistas-y-para-teorias-de-la-conspiracion.html\"\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(f\"✅ Conexion exitosa a {url}\")\n",
    "    \n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "parrafos = soup.find_all(\"p\")\n",
    "texto = \" \".join([p.text for p in parrafos])\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Realiza la solicitud al modelo de chat\n",
    "respuesta = openai.ChatCompletion.create(\n",
    "    model='gpt-3.5-turbo',\n",
    "    messages=[\n",
    "        {'role': 'system', 'content': 'Eres un periodista que resume noticias'},\n",
    "        {'role': 'user', 'content': texto}\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Imprime la respuesta del modelo\n",
    "print(respuesta.choices[0].message['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Taller de Web Scraping**\n",
    "\n",
    "## **Parte 1: Introducci√≥n a Web Scraping**\n",
    "En este taller, aprender√°s a realizar web scraping utilizando Python y la librer√≠a `BeautifulSoup`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **Ejercicio 1: Explorar el archivo robots.txt**\n",
    "\n",
    "1. Busca el archivo `robots.txt` de una p√°gina web y analiza sus reglas.\n",
    "   - Ejemplo: [https://www.wikipedia.org/robots.txt](https://www.wikipedia.org/robots.txt)\n",
    "   - Identifica qu√© partes est√°n permitidas para el scraping.\n",
    "\n",
    "## Explorar el archivo Robot.txt\n",
    "\n",
    "Dentro del archivo `robot.txt` podemos encontrar e identificar las partes que estan permitidas y cuales, para poder realizar los procesos de WebScraping. En este formato encontramos diferentes reglas relacionadas tales que:\n",
    "\n",
    "* **User-agent:** Indica a que rastreadores(como Googlebot o scrapers personalizados) se aplican las reglas.\n",
    "* **Disallow:** Especifica que partes del sitio estan prohibidas para el web scraping.\n",
    "* **Allow:** Especifica que partes del sitio estan permitidas para el scraping.\n",
    "\n",
    "### Preguntas reflexivas\n",
    "+ ¬øPor qu√© algunos sitios web bloquean el Web Scraping?\n",
    "    + El bloqueo del web scraping se da por diferentes razones, como sobrecarga del servidor, violacion de los terminos de servicio, o uso excesivo de recursos.\n",
    "    + **Sobrecarga del servidor**\n",
    "        + Los robots mal disenados pueden provocar el servidor al realizar solicitudes excesivas.\n",
    "    + **Violacion de los terminos de servicio**\n",
    "        + Los sitios web pueden bloquear los raspadores web por que violan los terminos de servicio de los sitios web.\n",
    "    + **Medidas anti-scraping**\n",
    "        +Los sitios web pueden implementar CAPTCHA para diferencia entre usuarios humanos y robot de scraping.\n",
    "\n",
    "+ ¬øCu√°ndo es preferible usar una API en lugar de Web Scraping?\n",
    "    + El uso de una API o de web scraping depende de la necesidad de los datos, el presupuesto, los recursos tecnologicos, y si el sitio web tiene un API.\n",
    "    + **API**\n",
    "        + Es una opcion para tener los datos estrucuturados y confiables\n",
    "        + Podemos integrar servicios de otros proveedores, como redes sociales, sistemas de pago, y geolocalizacion.\n",
    "        + Acelera el desarrollo de aplicaciones y facilita la automatizacion de tareas.\n",
    "\n",
    "    + **Web Scraping**\n",
    "        + Ofrece mayor flexibilidad y cobertura\n",
    "        + Permite extraer datos de sitios web que no tiene APIs\n",
    "        + Permite extraer informacion extra que no nos ofrece una API\n",
    "            \n",
    "+ Herramientas populares para Web Scraping en Python.\n",
    "    + La principal herramienta para el web Scraping en Python es la libreria de `Beautiful Soup`. Esta nos facilita el analisis y extraccion de datos, documentos tanto HTML y XML.\n",
    "\n",
    "### **robots.txt for www.mercadolibre.com.co**\n",
    "\n",
    "```Python\n",
    "User-agent: Amazonbot\n",
    "Disallow: /\n",
    "\n",
    "User-agent: ClaudeBot\n",
    "Disallow: /\n",
    "\n",
    "User-agent: FacebookExternalHit\n",
    "User-agent: FacebookBot\n",
    "User-agent: Twitterbot\n",
    "User-agent: LinkedInBot\n",
    "Disallow: \n",
    "\n",
    "User-agent: *\n",
    "Disallow: /HOME/\n",
    "Disallow: /gz/merch/\n",
    "Disallow: /gz/menu\n",
    "Disallow: /gz/webdevice/config\n",
    "Disallow: /gz/referidos\n",
    "Disallow: /*www.siteinfo.cf\n",
    "Disallow: /gz/cart/\n",
    "Disallow: /gz/checkout/\n",
    "Disallow: /gz/user-logged\n",
    "Disallow: /gz/shipping-selector\n",
    "Disallow: /gz/navigation/searches/last\n",
    "Disallow: /perfil/vendedor/\n",
    "Disallow: /perfil/comprador/\n",
    "Disallow: /perfil/profile/\n",
    "Disallow: /perfil/jm/profile\n",
    "Disallow: /perfil/ALEXSETHMS\n",
    "Disallow: /noindex/\n",
    "Disallow: /navigation/\n",
    "Disallow: /*itemid\n",
    "Disallow: /*/jm/item\n",
    "Disallow: /recommendations*\n",
    "Disallow: /*attributes=\n",
    "Disallow: /*quantity=\n",
    "Disallow: /org-img/html/\n",
    "Disallow: /registration?confirmation_url*\n",
    "Disallow: /home/recommendations\n",
    "Disallow: /social/\n",
    "Disallow: /adn/api*\n",
    "Disallow: /product-fe-recommendations/recommendations*\n",
    "Disallow: /*.js\n",
    "Disallow: /finditem.ml\n",
    "```\n",
    "\n",
    "### Analisis del archivo `robots.txt` de Mercado Libre\n",
    "\n",
    "1. **Bloqueo de bots especificos**\n",
    "```\n",
    "User-agent: Amazonbot\n",
    "Disallow: /\n",
    "User-agent: ClaudeBot\n",
    "Disallow: /\n",
    "\n",
    "```\n",
    "+ **Amazonbot** y **ClaudeBot** esta bloqueado, lo que significa que no puede rasrear ninguna parte del sitio.\n",
    "\n",
    "2. **Bots de redes sociales permitidos**\n",
    "```\n",
    "User-agent: FacebookExternalHit\n",
    "User-agent: FacebookBot\n",
    "User-agent: Twitterbot\n",
    "User-agent: LinkedInBot\n",
    "Disallow: \n",
    "```\n",
    "3. **Restricciones generales para todos los robots**\n",
    "```\n",
    "User-agent: *\n",
    "Disallow: /HOME/\n",
    "Disallow: /gz/merch/\n",
    "Disallow: /gz/menu\n",
    "Disallow: /gz/webdevice/config\n",
    "Disallow: /gz/referidos\n",
    "Disallow: /*www.siteinfo.cf\n",
    "Disallow: /gz/cart/\n",
    "Disallow: /gz/checkout/\n",
    "Disallow: /gz/user-logged\n",
    "Disallow: /gz/shipping-selector\n",
    "Disallow: /gz/navigation/searches/last\n",
    "Disallow: /perfil/vendedor/\n",
    "Disallow: /perfil/comprador/\n",
    "Disallow: /perfil/profile/\n",
    "Disallow: /perfil/jm/profile\n",
    "Disallow: /perfil/ALEXSETHMS\n",
    "Disallow: /noindex/\n",
    "Disallow: /navigation/\n",
    "Disallow: /*itemid\n",
    "Disallow: /*/jm/item\n",
    "Disallow: /recommendations*\n",
    "Disallow: /*attributes=\n",
    "Disallow: /*quantity=\n",
    "Disallow: /org-img/html/\n",
    "Disallow: /registration?confirmation_url*\n",
    "Disallow: /home/recommendations\n",
    "Disallow: /social/\n",
    "Disallow: /adn/api*\n",
    "Disallow: /product-fe-recommendations/recommendations*\n",
    "Disallow: /*.js\n",
    "Disallow: /finditem.ml\n",
    "```\n",
    "+ Se prohible el acceso a secciones sensibles como:\n",
    "    + Carrito de compras y checkout(`/gz/cart/`, `/gz/checkout/`)\n",
    "    + Paginas de usuarios (`/perfil/vendedor/`, `/perfil/comprador/`, etc)\n",
    "    + Paginas de navegacion y busqueda interna (`/navigation/`,`/gz/navigation/searchez/last`)\n",
    "    + Recomendaciones y API internas  (`/recommendations*`, `/adn/api*`, `/product-fe-recommendations/recommendations*`)\n",
    "\n",
    "+ Mercado libre restringe el acceso a datos sensibles y dinamicos\n",
    "+ Los bots de redes sociales tienen acceso total, lo que facilita compartir y promocionar productos\n",
    "+ El bloqueo de robots especificos como Amazon o Claude, posiblemente sera para evitar que lean y analisen el mercado con esta informacion.\n",
    "+ Al estar bloqueado el acceso a los archivos `.js` y las URLs de configuracion interna, se protegen de que el sitio sea replicado o que se analize con profundidad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Parte 2: Implementaci√≥n de Web Scraping en Python**\n",
    "\n",
    "### **Ejercicio 2: Extraer datos de una p√°gina web**\n",
    "\n",
    "1. Utiliza `requests` y `BeautifulSoup` para extraer contenido de una p√°gina web.\n",
    "2. Identifica elementos HTML relevantes (como etiquetas `<div>`, `<p>`, `<a>`).\n",
    "3. Extrae y almacena la informaci√≥n en un formato estructurado (CSV o JSON).\n",
    "\n",
    "**C√≥digo base:**\n",
    "```python\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://ejemplo.com\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Extraer informaci√≥n relevante\n",
    "data = soup.find_all(\"p\")\n",
    "for item in data:\n",
    "    print(item.text)\n",
    "```\n",
    "Vamos a realizar la extraccion de la informacion de la pagina de noticias `elpais.com` previa revision del documento `robots.txt` y no encontrar ninguna restriccion frente a esta practica a los robots de pruebas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install requests beautifulsoup4\n",
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Conexion exitosa a https://elpais.com/tecnologia/\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL de la pagina de noticias\n",
    "url = \"https://elpais.com/tecnologia/\"\n",
    "headers = {\"User-Agent\": \"Chrome/120.0.0.0\"}\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(f\"‚úÖ Conexion exitosa a {url}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üëΩ Cantidad de noticias extraidas 21\n",
      "1. Descifrando el consumo de agua de la IA: as√≠ oculta Amazon cu√°nto bebe su nube en Espa√±a\n",
      "2. El Gobierno propone blindar a los actores contra la inteligencia artificial\n",
      "3. Por qu√© Deepseek y Bluesky son las primeras grietas en el poder de la tecnocasta\n",
      "4. Las mafias del cibercrimen cuentan con un ej√©rcito de m√°s de 250.000 esclavos sometidos a torturas, extorsiones y violaciones\n",
      "5. Un port√°til solar, lentillas inteligentes y otras extravagancias del MWC\n",
      "6. Samsung expone c√≥mo la IA cambiar√° la forma de usar el ‚Äòsmartphone‚Äô\n",
      "7. Emilio Carrizosa, matem√°tico: ‚ÄúEl liderazgo de la inteligencia artificial no lo tienen ahora mismo los gobiernos, sino empresas privadas‚Äù\n",
      "8. Protecci√≥n de Datos impone una multa de un mill√≥n de euros a LaLiga por recoger datos biom√©tricos de los espectadores\n",
      "9. C√≥mo la pol√≠tica de Trump sobre IA beneficiar√° a las grandes empresas\n",
      "10. Los memes son una herramienta fundamental para las comunidades extremistas y para teor√≠as de la conspiraci√≥n\n",
      "11. Los robots del MWC se apuntan a la inteligencia artificial\n",
      "12. ‚ÄúLa guerra supone estr√©s constante para todos‚Äù: j√≥venes ucranios desarrollan tecnolog√≠a para tratar la salud mental y fomentar la natalidad\n",
      "13. El miedo al hombre blandengue vuelve a Silicon Valley\n",
      "14. Los m√≥viles del MWC 2025, cada vez menos y menos deslumbrantes\n",
      "15. ‚ÄúYo vivo de mi voz. Si me la emulan, estoy acabado‚Äù: Los actores de doblaje se movilizan contra la IA\n",
      "16. ‚ÄòFachatubers‚Äô: la extrema derecha se cuela en las pantallas de los m√°s j√≥venes\n",
      "17. ¬øQui√©n va ganando la carrera de la IA generativa?\n",
      "18. Oda al Autotune: himnos para la generaci√≥n de cristal\n",
      "19. El legionario terraplanista de Masterchef y otros `despertares de la conciencia¬¥\n",
      "20. P√≥dcast | El apote√≥sico final de 'Titania', la ficci√≥n sonora m√°s premiada del a√±o\n",
      "21. C√≥mo descartar definitivamente que Brad Pitt no es quien habla por videollamada, aunque lo parezca\n"
     ]
    }
   ],
   "source": [
    "# Parseamos el contenido de la pagina con BeatifulSoup\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Tras previa busqueda de los elementos HTML que contienen los titulos de las noticias\n",
    "# Estraemos los titulos con el tag h2\n",
    "titulos = soup.find_all(\"h2\") # nos devuelve una lista con todos los titulos\n",
    "\n",
    "print(f\"üëΩ Cantidad de noticias extraidas {len(titulos)}\")\n",
    "# Iteramos la lista de titulos\n",
    "for i,titulo in enumerate(titulos,start=1):\n",
    "    print(f\"{i}. {titulo.text.strip()}\") # Strip() nos ayuda a eliminar espacios al inico y al final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Extraemos las noticias que contengan algo relacionado a la IA\n",
      "1. Samsung expone c√≥mo la IA cambiar√° la forma de usar el ‚Äòsmartphone‚Äô\n",
      "2. C√≥mo la pol√≠tica de Trump sobre IA beneficiar√° a las grandes empresas\n",
      "3. ¬øQui√©n va ganando la carrera de la IA generativa?\n"
     ]
    }
   ],
   "source": [
    "print(f\"ü§ñ Extraemos las noticias que contengan algo relacionado a la IA\")\n",
    "i = 0\n",
    "for titulo in titulos:\n",
    "    if \" IA \" in titulo.text.upper() or \"inteligencia artificial\" in titulo.text.upper():\n",
    "        i += 1\n",
    "        print(f\"{i}. {titulo.text.strip()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Parte 3: Scraping de Datos en Tablas**\n",
    "\n",
    "### Conceptos clave\n",
    "\n",
    "- **Uso de `find_all` para extraer datos tabulares**: En esta secci√≥n, aprender√°s a extraer datos de tablas HTML utilizando la funci√≥n `find_all` de BeautifulSoup. Las tablas en HTML est√°n estructuradas con etiquetas como `<table>`, `<tr>` (filas), `<th>` (encabezados) y `<td>` (celdas de datos).\n",
    "\n",
    "### Ejercicio 3: Extraer datos de una tabla de Wikipedia\n",
    "\n",
    "1. **Busca una p√°gina de Wikipedia con una tabla de datos**: Por ejemplo, puedes usar la p√°gina de la lista de pa√≠ses por PIB nominal: [Lista de pa√≠ses por PIB (nominal)](https://es.wikipedia.org/wiki/Lista_de_pa%C3%ADses_por_PIB_(nominal)).\n",
    "\n",
    "2. **Modifica el siguiente c√≥digo para extraer los datos de una tabla espec√≠fica**:\n",
    "\n",
    "   ```python\n",
    "   import requests\n",
    "   from bs4 import BeautifulSoup\n",
    "\n",
    "   # URL de la p√°gina de Wikipedia con la tabla\n",
    "   url = \"https://es.wikipedia.org/wiki/Lista_de_pa%C3%ADses_por_PIB_(nominal)\"\n",
    "   \n",
    "   # Realizar la solicitud HTTP\n",
    "   response = requests.get(url)\n",
    "   \n",
    "   # Parsear el contenido HTML con BeautifulSoup\n",
    "   soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "   \n",
    "   # Encontrar la tabla por su clase (en este caso, \"wikitable\")\n",
    "   tabla = soup.find(\"table\", {\"class\": \"wikitable\"})\n",
    "   \n",
    "   # Extraer todas las filas de la tabla\n",
    "   filas = tabla.find_all(\"tr\")\n",
    "   \n",
    "   # Iterar sobre cada fila y extraer los datos de las celdas\n",
    "   for fila in filas:\n",
    "       columns = fila.find_all(\"td\")\n",
    "       datos = [col.text.strip() for col in columns]\n",
    "       print(datos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Conexion exitosa a https://es.wikipedia.org/wiki/Poblaci%C3%B3n_mundial\n"
     ]
    }
   ],
   "source": [
    "# URL de la pagina de wikipedia\n",
    "url = \"https://es.wikipedia.org/wiki/Poblaci%C3%B3n_mundial\"\n",
    "headers = {\"User-Agent\": \"Chrome/120.0.0.0\"}\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(f\"‚úÖ Conexion exitosa a {url}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üëΩ Cantidad de tablas extraidas 19\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Puesto</td>\n",
       "      <td>Pa√≠s o territorio dependiente</td>\n",
       "      <td>Poblaci√≥n  (proyecci√≥n 2024)[15]‚Äã</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.¬∫</td>\n",
       "      <td>India</td>\n",
       "      <td>1 450 935 779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.¬∫</td>\n",
       "      <td>China</td>\n",
       "      <td>1 419 321 279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.¬∫</td>\n",
       "      <td>Estados Unidos</td>\n",
       "      <td>345 426 567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.¬∫</td>\n",
       "      <td>Indonesia</td>\n",
       "      <td>283 487 932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.¬∫</td>\n",
       "      <td>Pakist√°n</td>\n",
       "      <td>251 269 158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6.¬∫</td>\n",
       "      <td>Nigeria</td>\n",
       "      <td>232 679 482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7.¬∫</td>\n",
       "      <td>Brasil</td>\n",
       "      <td>211 998 564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8.¬∫</td>\n",
       "      <td>Banglad√©s</td>\n",
       "      <td>173 562 367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9.¬∫</td>\n",
       "      <td>Rusia</td>\n",
       "      <td>144 820 423</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0                              1                                  2\n",
       "0  Puesto  Pa√≠s o territorio dependiente  Poblaci√≥n  (proyecci√≥n 2024)[15]‚Äã\n",
       "1     1.¬∫                          India                      1 450 935 779\n",
       "2     2.¬∫                          China                      1 419 321 279\n",
       "3     3.¬∫                 Estados Unidos                        345 426 567\n",
       "4     4.¬∫                      Indonesia                        283 487 932\n",
       "5     5.¬∫                       Pakist√°n                        251 269 158\n",
       "6     6.¬∫                        Nigeria                        232 679 482\n",
       "7     7.¬∫                         Brasil                        211 998 564\n",
       "8     8.¬∫                      Banglad√©s                        173 562 367\n",
       "9     9.¬∫                          Rusia                        144 820 423"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Parseamos el contenido de la pagina con BeatifulSoup\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "tablas = soup.find_all(\"table\",{\"class\":\"wikitable\"}) \n",
    "\n",
    "print(f\"üëΩ Cantidad de tablas extraidas {len(tablas)}\")\n",
    "\n",
    "tabla_objetivo = tablas[3]\n",
    "\n",
    "titulos = tabla_objetivo.find_all(\"th\")\n",
    "filas = tabla_objetivo.find_all(\"tr\")\n",
    "\n",
    "datos = []\n",
    "\n",
    "fila_titulos = [tl.text.strip() for tl in titulos]\n",
    "datos.append(fila_titulos)\n",
    "\n",
    "for fila in filas:\n",
    "    columnas = fila.find_all(\"td\")\n",
    "    fila_datos = [col.text.strip() for col in columnas]\n",
    "    if fila_datos:\n",
    "        datos.append(fila_datos)\n",
    "\n",
    "df = pd.DataFrame(datos)\n",
    "\n",
    "df.to_csv(\"paises_sur_america.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "# Para este caso practico traemos los 10 paises con mas poblacion\n",
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **Ejercicio 3: Scraping avanzado con Selenium**\n",
    "\n",
    "1. Usa `Selenium` para interactuar con p√°ginas din√°micas.\n",
    "2. Extrae informaci√≥n que se carga mediante JavaScript.\n",
    "3. Simula acciones de usuario, como clics y desplazamientos.\n",
    "\n",
    "**C√≥digo base:**\n",
    "```python\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "\n",
    "# Configurar el driver\n",
    "options = webdriver.ChromeOptions()\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "driver.get(\"https://ejemplo.com\")\n",
    "time.sleep(5)  # Esperar carga de la p√°gina\n",
    "\n",
    "data = driver.find_element(By.TAG_NAME, \"p\").text\n",
    "print(data)\n",
    "\n",
    "driver.quit()\n",
    "```\n",
    "\n",
    "## **Parte 3: √âtica y buenas pr√°cticas en Web Scraping**\n",
    "\n",
    "### **Ejercicio 4: Respetar los t√©rminos de uso y evitar bloqueos**\n",
    "- Revisar el `robots.txt` antes de hacer scraping.\n",
    "- Implementar tiempos de espera (`time.sleep()`) para no sobrecargar servidores.\n",
    "- Usar `headers` para simular una petici√≥n leg√≠tima.\n",
    "- Preferir APIs cuando est√©n disponibles.\n",
    "\n",
    "**Ejemplo de petici√≥n con `headers`:**\n",
    "```python\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "response = requests.get(url, headers=headers)\n",
    "```\n",
    "\n",
    "### **Ejercicio 5: Guardar y analizar los datos extra√≠dos**\n",
    "1. Almacenar los datos en CSV o JSON.\n",
    "2. Analizar la informaci√≥n con `pandas`.\n",
    "\n",
    "**Ejemplo de almacenamiento en CSV:**\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\"Datos\": data})\n",
    "df.to_csv(\"datos.csv\", index=False)\n",
    "```\n",
    "\n",
    "## **Conclusi√≥n**\n",
    "Este taller proporciona una introducci√≥n a Web Scraping con Python, cubriendo desde la exploraci√≥n de `robots.txt` hasta la implementaci√≥n con `BeautifulSoup` y `Selenium`. ¬°Ahora es tu turno de practicar! üöÄ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estructura de un Documento Markdown\n",
    "\n",
    "Markdown es un lenguaje de marcado ligero que permite formatear texto de manera sencilla. A continuaci√≥n, se presenta la estructura b√°sica de un documento en Markdown.\n",
    "\n",
    "## 1. Encabezados\n",
    "\n",
    "Se utilizan `#` para definir encabezados de distintos niveles:\n",
    "\n",
    "```markdown\n",
    "# Encabezado de nivel 1\n",
    "## Encabezado de nivel 2\n",
    "### Encabezado de nivel 3\n",
    "#### Encabezado de nivel 4\n",
    "##### Encabezado de nivel 5\n",
    "###### Encabezado de nivel 6\n",
    "```\n",
    "\n",
    "## 2. P√°rrafos y Saltos de L√≠nea\n",
    "\n",
    "Para crear un p√°rrafo, simplemente escribe el texto dejando una l√≠nea en blanco entre p√°rrafos.\n",
    "Para forzar un salto de l√≠nea dentro de un p√°rrafo, a√±ade dos espacios al final de la l√≠nea.\n",
    "\n",
    "## 3. Estilos de Texto\n",
    "\n",
    "Puedes aplicar diferentes estilos al texto:\n",
    "\n",
    "```markdown\n",
    "**Negrita** (doble asterisco o doble guion bajo): **Ejemplo** o __Ejemplo__\n",
    "*Cursiva* (un asterisco o un guion bajo): *Ejemplo* o _Ejemplo_\n",
    "~~Tachado~~ (doble tilde): ~~Ejemplo~~\n",
    "```\n",
    "\n",
    "## 4. Listas\n",
    "\n",
    "### Listas no ordenadas\n",
    "Se crean usando `-`, `*` o `+`:\n",
    "\n",
    "```markdown\n",
    "- Elemento 1\n",
    "- Elemento 2\n",
    "  - Subelemento 2.1\n",
    "  - Subelemento 2.2\n",
    "```\n",
    "\n",
    "### Listas ordenadas\n",
    "Se crean usando n√∫meros seguidos de un punto:\n",
    "\n",
    "```markdown\n",
    "1. Elemento 1\n",
    "2. Elemento 2\n",
    "   1. Subelemento 2.1\n",
    "   2. Subelemento 2.2\n",
    "```\n",
    "\n",
    "## 5. Enlaces\n",
    "\n",
    "```markdown\n",
    "[Texto del enlace](https://ejemplo.com)\n",
    "```\n",
    "\n",
    "Ejemplo: [Visitar Google](https://www.google.com)\n",
    "\n",
    "## 6. Im√°genes\n",
    "\n",
    "```markdown\n",
    "![Texto alternativo](https://via.placeholder.com/150 \"T√≠tulo opcional\")\n",
    "```\n",
    "\n",
    "## 7. Citas\n",
    "\n",
    "Se crean usando `>` al inicio de la l√≠nea:\n",
    "\n",
    "```markdown\n",
    "> Esto es una cita en Markdown.\n",
    "```\n",
    "\n",
    "## 8. C√≥digo\n",
    "\n",
    "Para incluir c√≥digo en l√≠nea, usa comillas invertidas `` `c√≥digo` ``.\n",
    "Para bloques de c√≥digo, usa triple comilla invertida:\n",
    "\n",
    "```markdown\n",
    "```\n",
    "print(\"Hola, Markdown!\")\n",
    "```\n",
    "```\n",
    "\n",
    "## 9. Tablas\n",
    "\n",
    "```markdown\n",
    "| Encabezado 1 | Encabezado 2 | Encabezado 3 |\n",
    "|-------------|-------------|-------------|\n",
    "| Dato 1      | Dato 2      | Dato 3      |\n",
    "| Dato 4      | Dato 5      | Dato 6      |\n",
    "```\n",
    "\n",
    "## 10. L√≠neas Horizontales\n",
    "\n",
    "Se crean con tres guiones `---`, tres asteriscos `***` o tres guiones bajos `___`.\n",
    "\n",
    "```markdown\n",
    "---\n",
    "```\n",
    "\n",
    "## 11. Checkbox (Listas de Tareas)\n",
    "\n",
    "```markdown\n",
    "- [ ] Tarea pendiente\n",
    "- [x] Tarea completada\n",
    "```\n",
    "\n",
    "## Conclusi√≥n\n",
    "\n",
    "Markdown es una herramienta poderosa y f√°cil de usar para formatear texto de manera eficiente. Con esta gu√≠a, puedes comenzar a escribir documentos en Markdown de manera estructurada y organizada.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

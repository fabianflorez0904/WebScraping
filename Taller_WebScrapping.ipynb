{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Taller de Web Scraping**\n",
    "\n",
    "## **Parte 1: Introducción a Web Scraping**\n",
    "En este taller, aprenderás a realizar web scraping utilizando Python y la librería `BeautifulSoup`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **Ejercicio 1: Explorar el archivo robots.txt**\n",
    "\n",
    "1. Busca el archivo `robots.txt` de una página web y analiza sus reglas.\n",
    "   - Ejemplo: [https://www.wikipedia.org/robots.txt](https://www.wikipedia.org/robots.txt)\n",
    "   - Identifica qué partes están permitidas para el scraping.\n",
    "\n",
    "## Explorar el archivo Robot.txt\n",
    "\n",
    "Dentro del archivo `robot.txt` podemos encontrar e identificar las partes que estan permitidas y cuales, para poder realizar los procesos de WebScraping. En este formato encontramos diferentes reglas relacionadas tales que:\n",
    "\n",
    "* **User-agent:** Indica a que rastreadores(como Googlebot o scrapers personalizados) se aplican las reglas.\n",
    "* **Disallow:** Especifica que partes del sitio estan prohibidas para el web scraping.\n",
    "* **Allow:** Especifica que partes del sitio estan permitidas para el scraping.\n",
    "\n",
    "### Preguntas reflexivas\n",
    "+ ¿Por qué algunos sitios web bloquean el Web Scraping?\n",
    "    + El bloqueo del web scraping se da por diferentes razones, como sobrecarga del servidor, violacion de los terminos de servicio, o uso excesivo de recursos.\n",
    "    + **Sobrecarga del servidor**\n",
    "        + Los robots mal disenados pueden provocar el servidor al realizar solicitudes excesivas.\n",
    "    + **Violacion de los terminos de servicio**\n",
    "        + Los sitios web pueden bloquear los raspadores web por que violan los terminos de servicio de los sitios web.\n",
    "    + **Medidas anti-scraping**\n",
    "        +Los sitios web pueden implementar CAPTCHA para diferencia entre usuarios humanos y robot de scraping.\n",
    "\n",
    "+ ¿Cuándo es preferible usar una API en lugar de Web Scraping?\n",
    "    + El uso de una API o de web scraping depende de la necesidad de los datos, el presupuesto, los recursos tecnologicos, y si el sitio web tiene un API.\n",
    "    + **API**\n",
    "        + Es una opcion para tener los datos estrucuturados y confiables\n",
    "        + Podemos integrar servicios de otros proveedores, como redes sociales, sistemas de pago, y geolocalizacion.\n",
    "        + Acelera el desarrollo de aplicaciones y facilita la automatizacion de tareas.\n",
    "\n",
    "    + **Web Scraping**\n",
    "        + Ofrece mayor flexibilidad y cobertura\n",
    "        + Permite extraer datos de sitios web que no tiene APIs\n",
    "        + Permite extraer informacion extra que no nos ofrece una API\n",
    "            \n",
    "+ Herramientas populares para Web Scraping en Python.\n",
    "    + La principal herramienta para el web Scraping en Python es la libreria de `Beautiful Soup`. Esta nos facilita el analisis y extraccion de datos, documentos tanto HTML y XML.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **robots.txt for www.mercadolibre.com.co**\n",
    "\n",
    "```Python\n",
    "User-agent: Amazonbot\n",
    "Disallow: /\n",
    "\n",
    "User-agent: ClaudeBot\n",
    "Disallow: /\n",
    "\n",
    "User-agent: FacebookExternalHit\n",
    "User-agent: FacebookBot\n",
    "User-agent: Twitterbot\n",
    "User-agent: LinkedInBot\n",
    "Disallow: \n",
    "\n",
    "User-agent: *\n",
    "Disallow: /HOME/\n",
    "Disallow: /gz/merch/\n",
    "Disallow: /gz/menu\n",
    "Disallow: /gz/webdevice/config\n",
    "Disallow: /gz/referidos\n",
    "Disallow: /*www.siteinfo.cf\n",
    "Disallow: /gz/cart/\n",
    "Disallow: /gz/checkout/\n",
    "Disallow: /gz/user-logged\n",
    "Disallow: /gz/shipping-selector\n",
    "Disallow: /gz/navigation/searches/last\n",
    "Disallow: /perfil/vendedor/\n",
    "Disallow: /perfil/comprador/\n",
    "Disallow: /perfil/profile/\n",
    "Disallow: /perfil/jm/profile\n",
    "Disallow: /perfil/ALEXSETHMS\n",
    "Disallow: /noindex/\n",
    "Disallow: /navigation/\n",
    "Disallow: /*itemid\n",
    "Disallow: /*/jm/item\n",
    "Disallow: /recommendations*\n",
    "Disallow: /*attributes=\n",
    "Disallow: /*quantity=\n",
    "Disallow: /org-img/html/\n",
    "Disallow: /registration?confirmation_url*\n",
    "Disallow: /home/recommendations\n",
    "Disallow: /social/\n",
    "Disallow: /adn/api*\n",
    "Disallow: /product-fe-recommendations/recommendations*\n",
    "Disallow: /*.js\n",
    "Disallow: /finditem.ml\n",
    "```\n",
    "\n",
    "### Analisis del archivo `robots.txt` de Mercado Libre\n",
    "\n",
    "1. **Bloqueo de bots especificos**\n",
    "```\n",
    "User-agent: Amazonbot\n",
    "Disallow: /\n",
    "User-agent: ClaudeBot\n",
    "Disallow: /\n",
    "\n",
    "```\n",
    "+ **Amazonbot** y **ClaudeBot** esta bloqueado, lo que significa que no puede rasrear ninguna parte del sitio.\n",
    "\n",
    "2. **Bots de redes sociales permitidos**\n",
    "```\n",
    "User-agent: FacebookExternalHit\n",
    "User-agent: FacebookBot\n",
    "User-agent: Twitterbot\n",
    "User-agent: LinkedInBot\n",
    "Disallow: \n",
    "```\n",
    "3. **Restricciones generales para todos los robots**\n",
    "```\n",
    "User-agent: *\n",
    "Disallow: /HOME/\n",
    "Disallow: /gz/merch/\n",
    "Disallow: /gz/menu\n",
    "Disallow: /gz/webdevice/config\n",
    "Disallow: /gz/referidos\n",
    "Disallow: /*www.siteinfo.cf\n",
    "Disallow: /gz/cart/\n",
    "Disallow: /gz/checkout/\n",
    "Disallow: /gz/user-logged\n",
    "Disallow: /gz/shipping-selector\n",
    "Disallow: /gz/navigation/searches/last\n",
    "Disallow: /perfil/vendedor/\n",
    "Disallow: /perfil/comprador/\n",
    "Disallow: /perfil/profile/\n",
    "Disallow: /perfil/jm/profile\n",
    "Disallow: /perfil/ALEXSETHMS\n",
    "Disallow: /noindex/\n",
    "Disallow: /navigation/\n",
    "Disallow: /*itemid\n",
    "Disallow: /*/jm/item\n",
    "Disallow: /recommendations*\n",
    "Disallow: /*attributes=\n",
    "Disallow: /*quantity=\n",
    "Disallow: /org-img/html/\n",
    "Disallow: /registration?confirmation_url*\n",
    "Disallow: /home/recommendations\n",
    "Disallow: /social/\n",
    "Disallow: /adn/api*\n",
    "Disallow: /product-fe-recommendations/recommendations*\n",
    "Disallow: /*.js\n",
    "Disallow: /finditem.ml\n",
    "```\n",
    "+ Se prohible el acceso a secciones sensibles como:\n",
    "    + Carrito de compras y checkout(`/gz/cart/`, `/gz/checkout/`)\n",
    "    + Paginas de usuarios (`/perfil/vendedor/`, `/perfil/comprador/`, etc)\n",
    "    + Paginas de navegacion y busqueda interna (`/navigation/`,`/gz/navigation/searchez/last`)\n",
    "    + Recomendaciones y API internas  (`/recommendations*`, `/adn/api*`, `/product-fe-recommendations/recommendations*`)\n",
    "\n",
    "+ Mercado libre restringe el acceso a datos sensibles y dinamicos\n",
    "+ Los bots de redes sociales tienen acceso total, lo que facilita compartir y promocionar productos\n",
    "+ El bloqueo de robots especificos como Amazon o Claude, posiblemente sera para evitar que lean y analisen el mercado con esta informacion.\n",
    "+ Al estar bloqueado el acceso a los archivos `.js` y las URLs de configuracion interna, se protegen de que el sitio sea replicado o que se analize con profundidad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Taller de Web Scraping**\n",
    "\n",
    "## **Parte 1: Introducción a Web Scraping**\n",
    "\n",
    "### **Ejercicio 1: Explorar el archivo robots.txt**\n",
    "\n",
    "1. Busca el archivo `robots.txt` de una página web y analiza sus reglas.\n",
    "   - Ejemplo: [https://www.wikipedia.org/robots.txt](https://www.wikipedia.org/robots.txt)\n",
    "   - Identifica qué partes están permitidas para el scraping.\n",
    "\n",
    "2. Explicación de las reglas:\n",
    "   - **User-agent**: Indica a qué rastreadores (como Googlebot o scrapers personalizados) se aplican las reglas.\n",
    "   - **Disallow**: Especifica qué partes del sitio están prohibidas para el web scraping.\n",
    "   - **Allow**: Especifica qué partes del sitio están permitidas para el scraping.\n",
    "\n",
    "### **Preguntas reflexivas**\n",
    "- ¿Por qué algunos sitios web bloquean el Web Scraping?\n",
    "- ¿Cuándo es preferible usar una API en lugar de Web Scraping?\n",
    "- Herramientas populares para Web Scraping en Python.\n",
    "\n",
    "## **Parte 2: Implementación de Web Scraping en Python**\n",
    "\n",
    "### **Ejercicio 2: Extraer datos de una página web**\n",
    "\n",
    "1. Utiliza `requests` y `BeautifulSoup` para extraer contenido de una página web.\n",
    "2. Identifica elementos HTML relevantes (como etiquetas `<div>`, `<p>`, `<a>`).\n",
    "3. Extrae y almacena la información en un formato estructurado (CSV o JSON).\n",
    "\n",
    "**Código base:**\n",
    "```python\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://ejemplo.com\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Extraer información relevante\n",
    "data = soup.find_all(\"p\")\n",
    "for item in data:\n",
    "    print(item.text)\n",
    "```\n",
    "\n",
    "### **Ejercicio 3: Scraping avanzado con Selenium**\n",
    "\n",
    "1. Usa `Selenium` para interactuar con páginas dinámicas.\n",
    "2. Extrae información que se carga mediante JavaScript.\n",
    "3. Simula acciones de usuario, como clics y desplazamientos.\n",
    "\n",
    "**Código base:**\n",
    "```python\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "\n",
    "# Configurar el driver\n",
    "options = webdriver.ChromeOptions()\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "driver.get(\"https://ejemplo.com\")\n",
    "time.sleep(5)  # Esperar carga de la página\n",
    "\n",
    "data = driver.find_element(By.TAG_NAME, \"p\").text\n",
    "print(data)\n",
    "\n",
    "driver.quit()\n",
    "```\n",
    "\n",
    "## **Parte 3: Ética y buenas prácticas en Web Scraping**\n",
    "\n",
    "### **Ejercicio 4: Respetar los términos de uso y evitar bloqueos**\n",
    "- Revisar el `robots.txt` antes de hacer scraping.\n",
    "- Implementar tiempos de espera (`time.sleep()`) para no sobrecargar servidores.\n",
    "- Usar `headers` para simular una petición legítima.\n",
    "- Preferir APIs cuando estén disponibles.\n",
    "\n",
    "**Ejemplo de petición con `headers`:**\n",
    "```python\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "response = requests.get(url, headers=headers)\n",
    "```\n",
    "\n",
    "### **Ejercicio 5: Guardar y analizar los datos extraídos**\n",
    "1. Almacenar los datos en CSV o JSON.\n",
    "2. Analizar la información con `pandas`.\n",
    "\n",
    "**Ejemplo de almacenamiento en CSV:**\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\"Datos\": data})\n",
    "df.to_csv(\"datos.csv\", index=False)\n",
    "```\n",
    "\n",
    "## **Conclusión**\n",
    "Este taller proporciona una introducción a Web Scraping con Python, cubriendo desde la exploración de `robots.txt` hasta la implementación con `BeautifulSoup` y `Selenium`. ¡Ahora es tu turno de practicar! 🚀\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estructura de un Documento Markdown\n",
    "\n",
    "Markdown es un lenguaje de marcado ligero que permite formatear texto de manera sencilla. A continuación, se presenta la estructura básica de un documento en Markdown.\n",
    "\n",
    "## 1. Encabezados\n",
    "\n",
    "Se utilizan `#` para definir encabezados de distintos niveles:\n",
    "\n",
    "```markdown\n",
    "# Encabezado de nivel 1\n",
    "## Encabezado de nivel 2\n",
    "### Encabezado de nivel 3\n",
    "#### Encabezado de nivel 4\n",
    "##### Encabezado de nivel 5\n",
    "###### Encabezado de nivel 6\n",
    "```\n",
    "\n",
    "## 2. Párrafos y Saltos de Línea\n",
    "\n",
    "Para crear un párrafo, simplemente escribe el texto dejando una línea en blanco entre párrafos.\n",
    "Para forzar un salto de línea dentro de un párrafo, añade dos espacios al final de la línea.\n",
    "\n",
    "## 3. Estilos de Texto\n",
    "\n",
    "Puedes aplicar diferentes estilos al texto:\n",
    "\n",
    "```markdown\n",
    "**Negrita** (doble asterisco o doble guion bajo): **Ejemplo** o __Ejemplo__\n",
    "*Cursiva* (un asterisco o un guion bajo): *Ejemplo* o _Ejemplo_\n",
    "~~Tachado~~ (doble tilde): ~~Ejemplo~~\n",
    "```\n",
    "\n",
    "## 4. Listas\n",
    "\n",
    "### Listas no ordenadas\n",
    "Se crean usando `-`, `*` o `+`:\n",
    "\n",
    "```markdown\n",
    "- Elemento 1\n",
    "- Elemento 2\n",
    "  - Subelemento 2.1\n",
    "  - Subelemento 2.2\n",
    "```\n",
    "\n",
    "### Listas ordenadas\n",
    "Se crean usando números seguidos de un punto:\n",
    "\n",
    "```markdown\n",
    "1. Elemento 1\n",
    "2. Elemento 2\n",
    "   1. Subelemento 2.1\n",
    "   2. Subelemento 2.2\n",
    "```\n",
    "\n",
    "## 5. Enlaces\n",
    "\n",
    "```markdown\n",
    "[Texto del enlace](https://ejemplo.com)\n",
    "```\n",
    "\n",
    "Ejemplo: [Visitar Google](https://www.google.com)\n",
    "\n",
    "## 6. Imágenes\n",
    "\n",
    "```markdown\n",
    "![Texto alternativo](https://via.placeholder.com/150 \"Título opcional\")\n",
    "```\n",
    "\n",
    "## 7. Citas\n",
    "\n",
    "Se crean usando `>` al inicio de la línea:\n",
    "\n",
    "```markdown\n",
    "> Esto es una cita en Markdown.\n",
    "```\n",
    "\n",
    "## 8. Código\n",
    "\n",
    "Para incluir código en línea, usa comillas invertidas `` `código` ``.\n",
    "Para bloques de código, usa triple comilla invertida:\n",
    "\n",
    "```markdown\n",
    "```\n",
    "print(\"Hola, Markdown!\")\n",
    "```\n",
    "```\n",
    "\n",
    "## 9. Tablas\n",
    "\n",
    "```markdown\n",
    "| Encabezado 1 | Encabezado 2 | Encabezado 3 |\n",
    "|-------------|-------------|-------------|\n",
    "| Dato 1      | Dato 2      | Dato 3      |\n",
    "| Dato 4      | Dato 5      | Dato 6      |\n",
    "```\n",
    "\n",
    "## 10. Líneas Horizontales\n",
    "\n",
    "Se crean con tres guiones `---`, tres asteriscos `***` o tres guiones bajos `___`.\n",
    "\n",
    "```markdown\n",
    "---\n",
    "```\n",
    "\n",
    "## 11. Checkbox (Listas de Tareas)\n",
    "\n",
    "```markdown\n",
    "- [ ] Tarea pendiente\n",
    "- [x] Tarea completada\n",
    "```\n",
    "\n",
    "## Conclusión\n",
    "\n",
    "Markdown es una herramienta poderosa y fácil de usar para formatear texto de manera eficiente. Con esta guía, puedes comenzar a escribir documentos en Markdown de manera estructurada y organizada.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

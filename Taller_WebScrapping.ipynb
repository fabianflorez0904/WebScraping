{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Taller de Web Scraping**\n",
    "\n",
    "## **Parte 1: Introducci√≥n a Web Scraping**\n",
    "En este taller, aprender√°s a realizar web scraping utilizando Python y la librer√≠a `BeautifulSoup`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **Ejercicio 1: Explorar el archivo robots.txt**\n",
    "\n",
    "1. Busca el archivo `robots.txt` de una p√°gina web y analiza sus reglas.\n",
    "   - Ejemplo: [https://www.wikipedia.org/robots.txt](https://www.wikipedia.org/robots.txt)\n",
    "   - Identifica qu√© partes est√°n permitidas para el scraping.\n",
    "\n",
    "## Explorar el archivo Robot.txt\n",
    "\n",
    "Dentro del archivo `robot.txt` podemos encontrar e identificar las partes que estan permitidas y cuales, para poder realizar los procesos de WebScraping. En este formato encontramos diferentes reglas relacionadas tales que:\n",
    "\n",
    "* **User-agent:** Indica a que rastreadores(como Googlebot o scrapers personalizados) se aplican las reglas.\n",
    "* **Disallow:** Especifica que partes del sitio estan prohibidas para el web scraping.\n",
    "* **Allow:** Especifica que partes del sitio estan permitidas para el scraping.\n",
    "\n",
    "### Preguntas reflexivas\n",
    "+ ¬øPor qu√© algunos sitios web bloquean el Web Scraping?\n",
    "    + El bloqueo del web scraping se da por diferentes razones, como sobrecarga del servidor, violacion de los terminos de servicio, o uso excesivo de recursos.\n",
    "    + **Sobrecarga del servidor**\n",
    "        + Los robots mal disenados pueden provocar el servidor al realizar solicitudes excesivas.\n",
    "    + **Violacion de los terminos de servicio**\n",
    "        + Los sitios web pueden bloquear los raspadores web por que violan los terminos de servicio de los sitios web.\n",
    "    + **Medidas anti-scraping**\n",
    "        +Los sitios web pueden implementar CAPTCHA para diferencia entre usuarios humanos y robot de scraping.\n",
    "\n",
    "+ ¬øCu√°ndo es preferible usar una API en lugar de Web Scraping?\n",
    "    + El uso de una API o de web scraping depende de la necesidad de los datos, el presupuesto, los recursos tecnologicos, y si el sitio web tiene un API.\n",
    "    + **API**\n",
    "        + Es una opcion para tener los datos estrucuturados y confiables\n",
    "        + Podemos integrar servicios de otros proveedores, como redes sociales, sistemas de pago, y geolocalizacion.\n",
    "        + Acelera el desarrollo de aplicaciones y facilita la automatizacion de tareas.\n",
    "\n",
    "    + **Web Scraping**\n",
    "        + Ofrece mayor flexibilidad y cobertura\n",
    "        + Permite extraer datos de sitios web que no tiene APIs\n",
    "        + Permite extraer informacion extra que no nos ofrece una API\n",
    "            \n",
    "+ Herramientas populares para Web Scraping en Python.\n",
    "    + La principal herramienta para el web Scraping en Python es la libreria de `Beautiful Soup`. Esta nos facilita el analisis y extraccion de datos, documentos tanto HTML y XML.\n",
    "\n",
    "### **robots.txt for www.mercadolibre.com.co**\n",
    "\n",
    "```Python\n",
    "User-agent: Amazonbot\n",
    "Disallow: /\n",
    "\n",
    "User-agent: ClaudeBot\n",
    "Disallow: /\n",
    "\n",
    "User-agent: FacebookExternalHit\n",
    "User-agent: FacebookBot\n",
    "User-agent: Twitterbot\n",
    "User-agent: LinkedInBot\n",
    "Disallow: \n",
    "\n",
    "User-agent: *\n",
    "Disallow: /HOME/\n",
    "Disallow: /gz/merch/\n",
    "Disallow: /gz/menu\n",
    "Disallow: /gz/webdevice/config\n",
    "Disallow: /gz/referidos\n",
    "Disallow: /*www.siteinfo.cf\n",
    "Disallow: /gz/cart/\n",
    "Disallow: /gz/checkout/\n",
    "Disallow: /gz/user-logged\n",
    "Disallow: /gz/shipping-selector\n",
    "Disallow: /gz/navigation/searches/last\n",
    "Disallow: /perfil/vendedor/\n",
    "Disallow: /perfil/comprador/\n",
    "Disallow: /perfil/profile/\n",
    "Disallow: /perfil/jm/profile\n",
    "Disallow: /perfil/ALEXSETHMS\n",
    "Disallow: /noindex/\n",
    "Disallow: /navigation/\n",
    "Disallow: /*itemid\n",
    "Disallow: /*/jm/item\n",
    "Disallow: /recommendations*\n",
    "Disallow: /*attributes=\n",
    "Disallow: /*quantity=\n",
    "Disallow: /org-img/html/\n",
    "Disallow: /registration?confirmation_url*\n",
    "Disallow: /home/recommendations\n",
    "Disallow: /social/\n",
    "Disallow: /adn/api*\n",
    "Disallow: /product-fe-recommendations/recommendations*\n",
    "Disallow: /*.js\n",
    "Disallow: /finditem.ml\n",
    "```\n",
    "\n",
    "### Analisis del archivo `robots.txt` de Mercado Libre\n",
    "\n",
    "1. **Bloqueo de bots especificos**\n",
    "```\n",
    "User-agent: Amazonbot\n",
    "Disallow: /\n",
    "User-agent: ClaudeBot\n",
    "Disallow: /\n",
    "\n",
    "```\n",
    "+ **Amazonbot** y **ClaudeBot** esta bloqueado, lo que significa que no puede rasrear ninguna parte del sitio.\n",
    "\n",
    "2. **Bots de redes sociales permitidos**\n",
    "```\n",
    "User-agent: FacebookExternalHit\n",
    "User-agent: FacebookBot\n",
    "User-agent: Twitterbot\n",
    "User-agent: LinkedInBot\n",
    "Disallow: \n",
    "```\n",
    "3. **Restricciones generales para todos los robots**\n",
    "```\n",
    "User-agent: *\n",
    "Disallow: /HOME/\n",
    "Disallow: /gz/merch/\n",
    "Disallow: /gz/menu\n",
    "Disallow: /gz/webdevice/config\n",
    "Disallow: /gz/referidos\n",
    "Disallow: /*www.siteinfo.cf\n",
    "Disallow: /gz/cart/\n",
    "Disallow: /gz/checkout/\n",
    "Disallow: /gz/user-logged\n",
    "Disallow: /gz/shipping-selector\n",
    "Disallow: /gz/navigation/searches/last\n",
    "Disallow: /perfil/vendedor/\n",
    "Disallow: /perfil/comprador/\n",
    "Disallow: /perfil/profile/\n",
    "Disallow: /perfil/jm/profile\n",
    "Disallow: /perfil/ALEXSETHMS\n",
    "Disallow: /noindex/\n",
    "Disallow: /navigation/\n",
    "Disallow: /*itemid\n",
    "Disallow: /*/jm/item\n",
    "Disallow: /recommendations*\n",
    "Disallow: /*attributes=\n",
    "Disallow: /*quantity=\n",
    "Disallow: /org-img/html/\n",
    "Disallow: /registration?confirmation_url*\n",
    "Disallow: /home/recommendations\n",
    "Disallow: /social/\n",
    "Disallow: /adn/api*\n",
    "Disallow: /product-fe-recommendations/recommendations*\n",
    "Disallow: /*.js\n",
    "Disallow: /finditem.ml\n",
    "```\n",
    "+ Se prohible el acceso a secciones sensibles como:\n",
    "    + Carrito de compras y checkout(`/gz/cart/`, `/gz/checkout/`)\n",
    "    + Paginas de usuarios (`/perfil/vendedor/`, `/perfil/comprador/`, etc)\n",
    "    + Paginas de navegacion y busqueda interna (`/navigation/`,`/gz/navigation/searchez/last`)\n",
    "    + Recomendaciones y API internas  (`/recommendations*`, `/adn/api*`, `/product-fe-recommendations/recommendations*`)\n",
    "\n",
    "+ Mercado libre restringe el acceso a datos sensibles y dinamicos\n",
    "+ Los bots de redes sociales tienen acceso total, lo que facilita compartir y promocionar productos\n",
    "+ El bloqueo de robots especificos como Amazon o Claude, posiblemente sera para evitar que lean y analisen el mercado con esta informacion.\n",
    "+ Al estar bloqueado el acceso a los archivos `.js` y las URLs de configuracion interna, se protegen de que el sitio sea replicado o que se analize con profundidad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Parte 2: Implementaci√≥n de Web Scraping en Python**\n",
    "\n",
    "### **Ejercicio 2: Extraer datos de una p√°gina web**\n",
    "\n",
    "1. Utiliza `requests` y `BeautifulSoup` para extraer contenido de una p√°gina web.\n",
    "2. Identifica elementos HTML relevantes (como etiquetas `<div>`, `<p>`, `<a>`).\n",
    "3. Extrae y almacena la informaci√≥n en un formato estructurado (CSV o JSON).\n",
    "\n",
    "**C√≥digo base:**\n",
    "```python\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://ejemplo.com\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Extraer informaci√≥n relevante\n",
    "data = soup.find_all(\"p\")\n",
    "for item in data:\n",
    "    print(item.text)\n",
    "```\n",
    "Vamos a realizar la extraccion de la informacion de la pagina de noticias `elpais.com` previa revision del documento `robots.txt` y no encontrar ninguna restriccion frente a esta practica a los robots de pruebas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3551064998.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[52]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mpip install requests beautifulsoup4\u001b[39m\n        ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "pip install requests beautifulsoup4\n",
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Conexion exitosa a https://elpais.com/tecnologia/\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL de la pagina de noticias\n",
    "url = \"https://elpais.com/tecnologia/\"\n",
    "headers = {\"User-Agent\": \"Chrome/120.0.0.0\"}\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(f\"‚úÖ Conexion exitosa a {url}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üëΩ Cantidad de noticias extraidas 21\n",
      "1. Descifrando el consumo de agua de la IA: as√≠ oculta Amazon cu√°nto bebe su nube en Espa√±a\n",
      "2. El Gobierno propone blindar a los actores contra la inteligencia artificial\n",
      "3. Por qu√© Deepseek y Bluesky son las primeras grietas en el poder de la tecnocasta\n",
      "4. Las mafias del cibercrimen cuentan con un ej√©rcito de m√°s de 250.000 esclavos sometidos a torturas, extorsiones y violaciones\n",
      "5. Un port√°til solar, lentillas inteligentes y otras extravagancias del MWC\n",
      "6. Samsung expone c√≥mo la IA cambiar√° la forma de usar el ‚Äòsmartphone‚Äô\n",
      "7. Emilio Carrizosa, matem√°tico: ‚ÄúEl liderazgo de la inteligencia artificial no lo tienen ahora mismo los gobiernos, sino empresas privadas‚Äù\n",
      "8. Protecci√≥n de Datos impone una multa de un mill√≥n de euros a LaLiga por recoger datos biom√©tricos de los espectadores\n",
      "9. C√≥mo la pol√≠tica de Trump sobre IA beneficiar√° a las grandes empresas\n",
      "10. Los memes son una herramienta fundamental para las comunidades extremistas y para teor√≠as de la conspiraci√≥n\n",
      "11. Los robots del MWC se apuntan a la inteligencia artificial\n",
      "12. ‚ÄúLa guerra supone estr√©s constante para todos‚Äù: j√≥venes ucranios desarrollan tecnolog√≠a para tratar la salud mental y fomentar la natalidad\n",
      "13. El miedo al hombre blandengue vuelve a Silicon Valley\n",
      "14. Los m√≥viles del MWC 2025, cada vez menos y menos deslumbrantes\n",
      "15. ‚ÄúYo vivo de mi voz. Si me la emulan, estoy acabado‚Äù: Los actores de doblaje se movilizan contra la IA\n",
      "16. ‚ÄòFachatubers‚Äô: la extrema derecha se cuela en las pantallas de los m√°s j√≥venes\n",
      "17. ¬øQui√©n va ganando la carrera de la IA generativa?\n",
      "18. Oda al Autotune: himnos para la generaci√≥n de cristal\n",
      "19. El legionario terraplanista de Masterchef y otros `despertares de la conciencia¬¥\n",
      "20. P√≥dcast | El apote√≥sico final de 'Titania', la ficci√≥n sonora m√°s premiada del a√±o\n",
      "21. C√≥mo descartar definitivamente que Brad Pitt no es quien habla por videollamada, aunque lo parezca\n"
     ]
    }
   ],
   "source": [
    "# Parseamos el contenido de la pagina con BeatifulSoup\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Tras previa busqueda de los elementos HTML que contienen los titulos de las noticias\n",
    "# Estraemos los titulos con el tag h2\n",
    "titulos = soup.find_all(\"h2\") # nos devuelve una lista con todos los titulos\n",
    "\n",
    "print(f\"üëΩ Cantidad de noticias extraidas {len(titulos)}\")\n",
    "# Iteramos la lista de titulos\n",
    "for i,titulo in enumerate(titulos,start=1):\n",
    "    print(f\"{i}. {titulo.text.strip()}\") # Strip() nos ayuda a eliminar espacios al inico y al final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Extraemos las noticias que contengan algo relacionado a la IA\n",
      "1. Samsung expone c√≥mo la IA cambiar√° la forma de usar el ‚Äòsmartphone‚Äô\n",
      "2. C√≥mo la pol√≠tica de Trump sobre IA beneficiar√° a las grandes empresas\n",
      "3. ¬øQui√©n va ganando la carrera de la IA generativa?\n"
     ]
    }
   ],
   "source": [
    "print(f\"ü§ñ Extraemos las noticias que contengan algo relacionado a la IA\")\n",
    "i = 0\n",
    "for titulo in titulos:\n",
    "    if \" IA \" in titulo.text.upper() or \"inteligencia artificial\" in titulo.text.upper():\n",
    "        i += 1\n",
    "        print(f\"{i}. {titulo.text.strip()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Parte 3: Scraping de Datos en Tablas**\n",
    "\n",
    "### Conceptos clave\n",
    "\n",
    "- **Uso de `find_all` para extraer datos tabulares**: En esta secci√≥n, aprender√°s a extraer datos de tablas HTML utilizando la funci√≥n `find_all` de BeautifulSoup. Las tablas en HTML est√°n estructuradas con etiquetas como `<table>`, `<tr>` (filas), `<th>` (encabezados) y `<td>` (celdas de datos).\n",
    "\n",
    "### Ejercicio 3: Extraer datos de una tabla de Wikipedia\n",
    "\n",
    "1. **Busca una p√°gina de Wikipedia con una tabla de datos**: Por ejemplo, puedes usar la p√°gina de la lista de pa√≠ses por PIB nominal: [Lista de pa√≠ses por PIB (nominal)](https://es.wikipedia.org/wiki/Lista_de_pa%C3%ADses_por_PIB_(nominal)).\n",
    "\n",
    "2. **Modifica el siguiente c√≥digo para extraer los datos de una tabla espec√≠fica**:\n",
    "\n",
    "   ```python\n",
    "   import requests\n",
    "   from bs4 import BeautifulSoup\n",
    "\n",
    "   # URL de la p√°gina de Wikipedia con la tabla\n",
    "   url = \"https://es.wikipedia.org/wiki/Lista_de_pa%C3%ADses_por_PIB_(nominal)\"\n",
    "   \n",
    "   # Realizar la solicitud HTTP\n",
    "   response = requests.get(url)\n",
    "   \n",
    "   # Parsear el contenido HTML con BeautifulSoup\n",
    "   soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "   \n",
    "   # Encontrar la tabla por su clase (en este caso, \"wikitable\")\n",
    "   tabla = soup.find(\"table\", {\"class\": \"wikitable\"})\n",
    "   \n",
    "   # Extraer todas las filas de la tabla\n",
    "   filas = tabla.find_all(\"tr\")\n",
    "   \n",
    "   # Iterar sobre cada fila y extraer los datos de las celdas\n",
    "   for fila in filas:\n",
    "       columns = fila.find_all(\"td\")\n",
    "       datos = [col.text.strip() for col in columns]\n",
    "       print(datos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Conexion exitosa a https://es.wikipedia.org/wiki/Poblaci%C3%B3n_mundial\n"
     ]
    }
   ],
   "source": [
    "# URL de la pagina de wikipedia\n",
    "url = \"https://es.wikipedia.org/wiki/Poblaci%C3%B3n_mundial\"\n",
    "headers = {\"User-Agent\": \"Chrome/120.0.0.0\"}\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(f\"‚úÖ Conexion exitosa a {url}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üëΩ Cantidad de tablas extraidas 19\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Puesto</td>\n",
       "      <td>Pa√≠s o territorio dependiente</td>\n",
       "      <td>Poblaci√≥n  (proyecci√≥n 2024)[15]‚Äã</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.¬∫</td>\n",
       "      <td>India</td>\n",
       "      <td>1 450 935 779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.¬∫</td>\n",
       "      <td>China</td>\n",
       "      <td>1 419 321 279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.¬∫</td>\n",
       "      <td>Estados Unidos</td>\n",
       "      <td>345 426 567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.¬∫</td>\n",
       "      <td>Indonesia</td>\n",
       "      <td>283 487 932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.¬∫</td>\n",
       "      <td>Pakist√°n</td>\n",
       "      <td>251 269 158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6.¬∫</td>\n",
       "      <td>Nigeria</td>\n",
       "      <td>232 679 482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7.¬∫</td>\n",
       "      <td>Brasil</td>\n",
       "      <td>211 998 564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8.¬∫</td>\n",
       "      <td>Banglad√©s</td>\n",
       "      <td>173 562 367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9.¬∫</td>\n",
       "      <td>Rusia</td>\n",
       "      <td>144 820 423</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0                              1                                  2\n",
       "0  Puesto  Pa√≠s o territorio dependiente  Poblaci√≥n  (proyecci√≥n 2024)[15]‚Äã\n",
       "1     1.¬∫                          India                      1 450 935 779\n",
       "2     2.¬∫                          China                      1 419 321 279\n",
       "3     3.¬∫                 Estados Unidos                        345 426 567\n",
       "4     4.¬∫                      Indonesia                        283 487 932\n",
       "5     5.¬∫                       Pakist√°n                        251 269 158\n",
       "6     6.¬∫                        Nigeria                        232 679 482\n",
       "7     7.¬∫                         Brasil                        211 998 564\n",
       "8     8.¬∫                      Banglad√©s                        173 562 367\n",
       "9     9.¬∫                          Rusia                        144 820 423"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Parseamos el contenido de la pagina con BeatifulSoup\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "tablas = soup.find_all(\"table\",{\"class\":\"wikitable\"}) \n",
    "\n",
    "print(f\"üëΩ Cantidad de tablas extraidas {len(tablas)}\")\n",
    "\n",
    "tabla_objetivo = tablas[3]\n",
    "\n",
    "titulos = tabla_objetivo.find_all(\"th\")\n",
    "filas = tabla_objetivo.find_all(\"tr\")\n",
    "\n",
    "datos = []\n",
    "\n",
    "fila_titulos = [tl.text.strip() for tl in titulos]\n",
    "datos.append(fila_titulos)\n",
    "\n",
    "for fila in filas:\n",
    "    columnas = fila.find_all(\"td\")\n",
    "    fila_datos = [col.text.strip() for col in columnas]\n",
    "    if fila_datos:\n",
    "        datos.append(fila_datos)\n",
    "\n",
    "df = pd.DataFrame(datos)\n",
    "\n",
    "df.to_csv(\"paises_sur_america.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "# Para este caso practico traemos los 10 paises con mas poblacion\n",
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Parte 4: Buenas Pr√°cticas y √âtica en Web Scraping**\n",
    "\n",
    "### Conceptos clave\n",
    "\n",
    "- **Respetar `robots.txt`**: El archivo `robots.txt` es un est√°ndar utilizado por los sitios web para indicar a los bots (como los de Web Scraping) qu√© partes del sitio pueden o no pueden ser accedidas. Es importante respetar estas reglas para evitar problemas legales o t√©cnicos.\n",
    "\n",
    "- **No sobrecargar servidores con muchas solicitudes en poco tiempo**: Realizar demasiadas solicitudes en un corto per√≠odo de tiempo puede sobrecargar el servidor del sitio web, lo que puede llevar a bloqueos o incluso a la denegaci√≥n de servicio. Es recomendable implementar pausas entre solicitudes.\n",
    "\n",
    "- **Usar cabeceras HTTP adecuadas (`User-Agent`)**: El `User-Agent` es una cabecera HTTP que identifica el navegador o herramienta que est√° realizando la solicitud. Es importante usar un `User-Agent` v√°lido para evitar ser bloqueado por el servidor.\n",
    "\n",
    "- **Preferir APIs oficiales cuando est√©n disponibles**: Si el sitio web ofrece una API oficial, es preferible usarla en lugar de realizar Web Scraping. Las APIs est√°n dise√±adas para proporcionar datos de manera estructurada y suelen ser m√°s eficientes y legales.\n",
    "\n",
    "### Ejercicio 4: Rotaci√≥n de User-Agent\n",
    "\n",
    "Modifica tu c√≥digo de BeautifulSoup para incluir un `User-Agent` diferente en cada solicitud. Aqu√≠ tienes un ejemplo de c√≥mo hacerlo:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.‚úÖ Conexion exitosa a https://elpais.com/tecnologia/\n",
      "1.‚úÖ Conexion exitosa a https://elpais.com/tecnologia/\n",
      "2.‚úÖ Conexion exitosa a https://elpais.com/tecnologia/\n",
      "3.‚úÖ Conexion exitosa a https://elpais.com/tecnologia/\n",
      "4.‚úÖ Conexion exitosa a https://elpais.com/tecnologia/\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "\n",
    "# Lista de User-Agents para rotar\n",
    "user_agents = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Safari/605.1.15\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0\",\n",
    "    \"Mozilla/5.0 (iPhone; CPU iPhone OS 14_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0 Mobile/15E148 Safari/604.1\"\n",
    "]\n",
    "\n",
    "# Seleccionar un User-Agent aleatorio\n",
    "headers = {\n",
    "    \"User-Agent\": random.choice(user_agents)\n",
    "}\n",
    "\n",
    "# URL de la p√°gina a scrapear\n",
    "url = \"https://elpais.com/tecnologia/\"\n",
    "\n",
    "# Realizar la solicitud HTTP con el User-Agent rotado\n",
    "\n",
    "for i in range(5):\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        print(f\"{i}.‚úÖ Conexion exitosa a {url}\")\n",
    "    else:\n",
    "        print(f\"{i}.‚ö†Ô∏è Conexion codigo a {response.status_code}\")\n",
    "        \n",
    "# Parsear el contenido HTML con BeautifulSoup\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preguntas\n",
    "+ ¬øPor qu√© es importante rotar User-Agents?\n",
    "\n",
    "    + Rotar el `User-Agent` es imporante para evitar que el servidor detecte que el webscraping que estamos realizando proviene de un robot y no nos vaya a bloquear los servicios o el acceso. Al cambiar de `User Agent` Simulamos que las solicitudes provienen de navegadores diferentes en cada peticion. \n",
    "\n",
    "\n",
    "+ ¬øQu√© pasa si realizamos muchas solicitudes a un sitio sin control?\n",
    "\n",
    "    +Si Realizamos muchas solicitudes a un sitio en un corto tiempo podemos afectar negativamente el servicio que estamos consultando.\n",
    "\n",
    "    - Podemos resultar con un bloqueo temporal o que nos beten permantente desde nuestra IP\n",
    "    - Puede resultar en un ataque de denegacion de servicios (DoS) para otros usuarios.\n",
    "    - Si la pagina o servicio consultado lo detecta como un ataque real pueden emprender acciones legales contra el ejecutante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Parte 5: Uso de IA en Web Scraping**\n",
    "\n",
    "### Conceptos clave\n",
    "\n",
    "- **¬øC√≥mo puede ayudar la IA en el an√°lisis de datos obtenidos mediante Web Scraping?**\n",
    "  La Inteligencia Artificial (IA) puede ser de gran ayuda para procesar y analizar grandes vol√∫menes de datos extra√≠dos mediante Web Scraping. Algunas aplicaciones incluyen:\n",
    "  - **Resumen autom√°tico de texto**: La IA puede resumir art√≠culos o noticias largas en pocas frases.\n",
    "  - **Clasificaci√≥n de contenido**: La IA puede clasificar datos en categor√≠as espec√≠ficas, como noticias por tem√°tica.\n",
    "  - **An√°lisis de sentimientos**: La IA puede determinar el tono emocional de un texto (positivo, negativo, neutral).\n",
    "  - **Extracci√≥n de entidades**: La IA puede identificar nombres, lugares, fechas y otros elementos clave en un texto.\n",
    "\n",
    "- **Introducci√≥n a herramientas de IA**:\n",
    "  - **GPT (Generative Pre-trained Transformer)**: Modelos de lenguaje como GPT-4 pueden generar texto, resumir contenido y realizar tareas de procesamiento de lenguaje natural (NLP).\n",
    "  - **Hugging Face**: Una plataforma que ofrece modelos preentrenados para tareas de NLP, como clasificaci√≥n de texto y generaci√≥n de res√∫menes.\n",
    "  - **Google Vision API**: Herramienta de Google para an√°lisis de im√°genes, que puede ser √∫til si el scraping incluye contenido visual.\n",
    "\n",
    "### Ejercicio 5: Resumen Autom√°tico de Contenido Web\n",
    "\n",
    "1. **Extraer el texto de un art√≠culo de noticias usando Web Scraping**.\n",
    "2. **Enviar el texto a la API de OpenAI (GPT-4) o Google Gemini para obtener un resumen**.\n",
    "3. **Comparar el resumen generado por IA con el contenido original**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: openai in c:\\users\\fabfl\\appdata\\roaming\\python\\python312\\site-packages (1.65.4)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\fabfl\\appdata\\roaming\\python\\python312\\site-packages (from openai) (4.8.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\fabfl\\appdata\\roaming\\python\\python312\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\fabfl\\appdata\\roaming\\python\\python312\\site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\fabfl\\appdata\\roaming\\python\\python312\\site-packages (from openai) (0.8.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\fabfl\\appdata\\roaming\\python\\python312\\site-packages (from openai) (2.10.6)\n",
      "Requirement already satisfied: sniffio in c:\\users\\fabfl\\appdata\\roaming\\python\\python312\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\fabfl\\appdata\\roaming\\python\\python312\\site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\fabfl\\appdata\\roaming\\python\\python312\\site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\fabfl\\appdata\\roaming\\python\\python312\\site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\fabfl\\appdata\\roaming\\python\\python312\\site-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\fabfl\\appdata\\roaming\\python\\python312\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\fabfl\\appdata\\roaming\\python\\python312\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\fabfl\\appdata\\roaming\\python\\python312\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\fabfl\\appdata\\roaming\\python\\python312\\site-packages (from pydantic<3,>=1.9.0->openai) (2.27.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\fabfl\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Conexion exitosa a https://elpais.com/tecnologia/2025-03-04/los-memes-son-una-herramienta-fundamental-para-las-comunidades-extremistas-y-para-teorias-de-la-conspiracion.html\n"
     ]
    },
    {
     "ename": "APIRemovedInV1",
     "evalue": "\n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAPIRemovedInV1\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 39\u001b[39m\n\u001b[32m     36\u001b[39m openai.api_key = os.getenv(\u001b[33m\"\u001b[39m\u001b[33mOPENAI_API_KEY\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# Realiza la solicitud al modelo de chat\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m respuesta = \u001b[43mopenai\u001b[49m\u001b[43m.\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mgpt-3.5-turbo\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msystem\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mEres un periodista que resume noticias\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtexto\u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# Imprime la respuesta del modelo\u001b[39;00m\n\u001b[32m     48\u001b[39m \u001b[38;5;28mprint\u001b[39m(respuesta.choices[\u001b[32m0\u001b[39m].message[\u001b[33m'\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\openai\\lib\\_old_api.py:39\u001b[39m, in \u001b[36mAPIRemovedInV1Proxy.__call__\u001b[39m\u001b[34m(self, *_args, **_kwargs)\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *_args: Any, **_kwargs: Any) -> Any:\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m APIRemovedInV1(symbol=\u001b[38;5;28mself\u001b[39m._symbol)\n",
      "\u001b[31mAPIRemovedInV1\u001b[39m: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import openai\n",
    "import random\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Cargar las variables de entorno desde el archivo .env\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "user_agents = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Safari/605.1.15\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0\",\n",
    "    \"Mozilla/5.0 (iPhone; CPU iPhone OS 14_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0 Mobile/15E148 Safari/604.1\"\n",
    "]\n",
    "\n",
    "# Seleccionar un User-Agent aleatorio\n",
    "headers = {\n",
    "    \"User-Agent\": random.choice(user_agents)\n",
    "}\n",
    "\n",
    "url = \"https://elpais.com/tecnologia/2025-03-04/los-memes-son-una-herramienta-fundamental-para-las-comunidades-extremistas-y-para-teorias-de-la-conspiracion.html\"\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(f\"‚úÖ Conexion exitosa a {url}\")\n",
    "    \n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "parrafos = soup.find_all(\"p\")\n",
    "texto = \" \".join([p.text for p in parrafos])\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Realiza la solicitud al modelo de chat\n",
    "respuesta = openai.ChatCompletion.create(\n",
    "    model='gpt-3.5-turbo',\n",
    "    messages=[\n",
    "        {'role': 'system', 'content': 'Eres un periodista que resume noticias'},\n",
    "        {'role': 'user', 'content': texto}\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Imprime la respuesta del modelo\n",
    "print(respuesta.choices[0].message['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
